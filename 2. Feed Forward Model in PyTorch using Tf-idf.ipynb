{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: **Creating the Corpus, Tf-idf, Linear Model** using `BucketIterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_12424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_46673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_61665</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034070_k141_24121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034071_k141_39888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Response                    BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "0  NonResponder  ERS2034069_k141_12424        0        0        0        0   \n",
       "1  NonResponder  ERS2034069_k141_46673        0        0        0        0   \n",
       "2  NonResponder  ERS2034069_k141_61665        0        0        0        0   \n",
       "3  NonResponder  ERS2034070_k141_24121        0        0        0        0   \n",
       "4  NonResponder  ERS2034071_k141_39888        0        0        0        0   \n",
       "\n",
       "   PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "0        0        0        0        0  ...        0        0        0   \n",
       "1        0        0        0        0  ...        0        0        0   \n",
       "2        0        0        0        0  ...        0        0        0   \n",
       "3        0        0        0        0  ...        0        0        0   \n",
       "4        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "   PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "0        0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DataFrames/final_pfam_dataframe_no_ID.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df[df.columns[2:]].sum()\n",
    "\n",
    "len(l[l>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.Response == 'Responder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.Response == 'NonResponder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus is a large Strings for all rows \n",
    "> One for Responder\n",
    "\n",
    "> Another one for NonResponder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_maker(df, response):\n",
    "    corpus = \"\"\n",
    "    cols = df.columns[2:]\n",
    "    for index in range(len(df)):\n",
    "        col_names = ' '\n",
    "        if df.loc[index, 'Response'] == response:\n",
    "            for col in cols:\n",
    "                if df.loc[index, col] == 1:\n",
    "                    col_names += \" \" + col\n",
    "            corpus += col_names + \".\"\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "responder = corpus_maker(df, 'Responder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  PF00005 PF08659 PF00501 PF00109 PF02801 PF00550 PF12833 PF00165 PF07690 PF00668 PF00664 PF13649 PF08241 PF00589 PF16197 PF00698 PF08242 PF00425 PF00108 PF06167 PF01135.  PF03176 PF02272 PF13561 PF00106 PF08659 PF00501 PF00109 PF02801 PF00550 PF13279 PF03061 PF03548 PF13723 PF13489 PF00891 PF03279 PF00221 PF00326 PF00535 PF01553 PF01738 PF07859 PF12837 PF00037 PF12838 PF13187 PF13237 PF12797 PF12800 PF08242 PF13481 PF01551 PF04060 PF02508 PF04205 PF03116 PF01411 PF07973 PF13375 PF01512 PF10531 PF03796 PF00892 PF00772 PF01784 PF02591 PF11306 PF04246.  PF13561 PF00106 PF08659 PF00501 PF00109 PF02801 PF00550 PF00975 PF13193 PF00668 PF00126 PF03466 PF01554 PF03721 PF00589 PF03446 PF00881 PF02771 PF00441 PF08028 PF16197 PF02737 PF00725 PF00698 PF08240 PF00107 PF01425 PF00144 PF08020 PF01709 PF13602 PF14765.  PF00512 PF00486 PF00072 PF03176 PF00501 PF00593 PF07715 PF05977 PF07690 PF01032 PF00975 PF00668 PF03621 PF00756 PF11806 PF14905 PF00924 PF00672 PF02321 PF11604 PF00529 PF16576 PF16572 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responder[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_responder = corpus_maker(df, 'NonResponder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  PF00005 PF00501 PF00550 PF01522 PF00975 PF13193 PF00668 PF01408 PF03773 PF03435 PF00145 PF02492 PF01475 PF01297 PF00950 PF07683.  PF03176 PF13561 PF00106 PF08659 PF00501 PF00109 PF02801 PF00550 PF00890 PF13450 PF01593 PF13279 PF03061 PF07977 PF03548 PF01522 PF13489 PF00891 PF00221 PF00535 PF03417 PF01553 PF13411 PF00376 PF09835 PF05175 PF13847 PF13649 PF08241 PF08242 PF01551 PF01177.  PF00501 PF00550 PF00668 PF03023 PF01554 PF14667 PF00702 PF01943 PF13412 PF12802 PF01325 PF03591 PF05437 PF08282 PF13581 PF01740 PF13466 PF12320 PF13558 PF00122 PF13630 PF07853 PF12840 PF01022 PF04464.  PF00501 PF00109 PF02801 PF00550 PF12833 PF00165 PF00975 PF13193 PF00668 PF01648 PF16197 PF12697.  PF03176 PF00654 PF13561 PF00106 PF08659 PF00109 PF02801 PF00550 PF00890 PF13450 PF01593 PF13279 PF03061 PF03548 PF01522 PF13723 PF08545 PF13489 PF00891 PF03279 PF00221 PF00535 PF01553 PF09835 PF05175 PF13847 PF13649 PF08241 PF00571 PF08242 PF03772 PF13567 PF00834 PF02911 PF00551 PF01300.  PF13561 PF00106 PF08'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_responder[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21948, 78804)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responder), len(non_responder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `responder` and `non_responder` are 2 long strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create articles\n",
    "- Each article is a string consisits of all column names which have value = 1\n",
    "- Each row represent 1 article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NonResponder articels\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sents=sent_tokenize(non_responder)\n",
    "for index, sent in enumerate(sents):\n",
    "    sent = sent.replace('.', \"\")\n",
    "    text_file = open(r\"corpus_new\\NonResponder\\%d.txt\" % index, \"w\")\n",
    "    text_file.write(sent)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Responder articels\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sents=sent_tokenize(responder)\n",
    "for index, sent in enumerate(sents):\n",
    "    sent = sent.replace('.', \"\")\n",
    "    text_file = open(r\"corpus_new\\Responder\\%d.txt\" % index, \"w\")\n",
    "    text_file.write(sent)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if there's any duplicate \n",
    "- There shouldn't be any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=sent_tokenize(responder)\n",
    "ll= []\n",
    "for index, sent in enumerate(sents):\n",
    "    l = []\n",
    "    sent = sent.replace('.', \"\")\n",
    "    l.append(sent.split(\" \"))\n",
    "    ll.extend(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PF13561': 1,\n",
       "         'PF00106': 1,\n",
       "         'PF08659': 1,\n",
       "         'PF00109': 1,\n",
       "         'PF02801': 1,\n",
       "         'PF00890': 1,\n",
       "         'PF13450': 1,\n",
       "         'PF01593': 1,\n",
       "         'PF13279': 1,\n",
       "         'PF03061': 1,\n",
       "         'PF08281': 1,\n",
       "         'PF00070': 1,\n",
       "         'PF03807': 1,\n",
       "         'PF04545': 1,\n",
       "         'PF00588': 1,\n",
       "         'PF01946': 1,\n",
       "         'PF13173': 1,\n",
       "         'PF00436': 1,\n",
       "         'PF01488': 1,\n",
       "         'PF13635': 1,\n",
       "         'PF11026': 1,\n",
       "         'PF08501': 1,\n",
       "         'PF00709': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(ll[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "DATA_DIR = \"./corpus_new/\"\n",
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\", random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data` is a bunch object consists of `'data'` and `'target'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 444 articles in our dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>PF09950 PF11681 PF11863 PF13262 PF04233 PF1420...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>PF13165 PF13353 PF04055 PF13450 PF00152 PF0133...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>PF03176 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>PF13165 PF13353 PF04055 PF00890 PF13450 PF0029...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0\n",
       "5  PF09950 PF11681 PF11863 PF13262 PF04233 PF1420...      0\n",
       "6  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      1\n",
       "7  PF13165 PF13353 PF04055 PF13450 PF00152 PF0133...      0\n",
       "8  PF03176 PF13561 PF00106 PF08659 PF00501 PF0010...      0\n",
       "9  PF13165 PF13353 PF04055 PF00890 PF13450 PF0029...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_corpus = pd.DataFrame(list(zip(data['data'], data['target'])), columns=['text', 'label'])\n",
    "print(f\"we have {len(df_corpus)} articles in our dataset\")\n",
    "df_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PF02518': 1,\n",
       "         'PF00512': 1,\n",
       "         'PF13561': 1,\n",
       "         'PF00106': 1,\n",
       "         'PF08659': 1,\n",
       "         'PF00501': 1,\n",
       "         'PF00109': 1,\n",
       "         'PF02801': 1,\n",
       "         'PF00550': 1,\n",
       "         'PF00890': 1,\n",
       "         'PF13450': 1,\n",
       "         'PF01593': 1,\n",
       "         'PF13279': 1,\n",
       "         'PF03061': 1,\n",
       "         'PF13970': 1,\n",
       "         'PF07977': 1,\n",
       "         'PF03548': 1,\n",
       "         'PF01522': 1,\n",
       "         'PF04383': 1,\n",
       "         'PF13723': 1,\n",
       "         'PF08545': 1,\n",
       "         'PF13489': 1,\n",
       "         'PF00891': 1,\n",
       "         'PF03279': 1,\n",
       "         'PF01370': 1,\n",
       "         'PF00221': 1,\n",
       "         'PF09190': 1,\n",
       "         'PF01406': 1,\n",
       "         'PF09334': 1,\n",
       "         'PF01553': 1,\n",
       "         'PF09835': 1,\n",
       "         'PF05175': 1,\n",
       "         'PF13847': 1,\n",
       "         'PF13649': 1,\n",
       "         'PF08241': 1,\n",
       "         'PF08447': 1,\n",
       "         'PF01590': 1,\n",
       "         'PF08242': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Let's look at the 1st row of data\n",
    "Counter(df_corpus.iloc[0,0].replace(\".\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(df_corpus.iloc[0,0].split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pfams in our dataset 12487\n"
     ]
    }
   ],
   "source": [
    "pfams  = []\n",
    "for txt in df_corpus[\"text\"]:\n",
    "    pfams.extend(txt.split(\" \"))\n",
    "    \n",
    "# Total number of pfams in our dataset\n",
    "print(f\"Total number of pfams in our dataset {len(pfams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique pfams in our dataset 1457\n"
     ]
    }
   ],
   "source": [
    "unique_pfams = list(set(pfams))\n",
    "\n",
    "print(f\"Total number of unique pfams in our dataset {len(unique_pfams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load the corpus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.to_csv(\"DataFrames/df_corpus_pfams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.read_csv(\"DataFrames/df_corpus_pfams.csv\")\n",
    "df_corpus.head()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corpus[ df_corpus.label==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corpus[df_corpus.label==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different rows has different length for `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 303)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corpus.iloc[1, :][0]), len(df_corpus.iloc[0, :][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of Responders and NonResponders matches in `df_corpus` and the original `df`\n",
    "\n",
    "> label 0 &#8594; NonResponder\n",
    "\n",
    "> Labe 1 &#8594; Responder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create 2 seperate dataframes for test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_corpus, test_size = 0.3, random_state = 42, stratify = df_corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "36   PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...      0\n",
       "411  PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...      1\n",
       "102  PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...      1\n",
       "135  PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...      1\n",
       "199  PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...      0"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24, 0.24)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(train[train.label == 1])/ len(train),2) ,round(len(test[test.label == 1])/len(test), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                  text  label\n",
       " 0    PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...      0\n",
       " 1    PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...      1\n",
       " 2    PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...      1\n",
       " 3    PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...      1\n",
       " 4    PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...      0\n",
       " ..                                                 ...    ...\n",
       " 305  PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...      0\n",
       " 306  PF13165 PF13353 PF04055 PF01276 PF00860 PF1280...      0\n",
       " 307  PF00072 PF00501 PF00109 PF02801 PF00550 PF1450...      1\n",
       " 308  PF07728 PF00004 PF00654 PF13561 PF00106 PF0865...      0\n",
       " 309  PF13165 PF13353 PF04055 PF13394 PF00528 PF0490...      1\n",
       " \n",
       " [310 rows x 2 columns],\n",
       "                                                   text  label\n",
       " 0    PF00072 PF00501 PF00550 PF00890 PF01370 PF0066...      0\n",
       " 1    PF00583 PF00717 PF13508 PF13673 PF11799 PF0081...      0\n",
       " 2    PF07728 PF00004 PF00654 PF00005 PF13561 PF0010...      1\n",
       " 3    PF00072 PF00005 PF00501 PF13450 PF13193 PF0277...      1\n",
       " 4    PF00218 PF00291 PF00290 PF02146 PF01649 PF0251...      1\n",
       " ..                                                 ...    ...\n",
       " 129  PF09950 PF11681 PF11863 PF13262 PF04233 PF1420...      0\n",
       " 130  PF01842 PF00501 PF07690 PF02775 PF00532 PF1337...      0\n",
       " 131  PF02518 PF00512 PF00072 PF00106 PF08659 PF0050...      0\n",
       " 132  PF07728 PF00004 PF00654 PF13561 PF00106 PF0865...      0\n",
       " 133  PF13165 PF13353 PF04055 PF13450 PF13186 PF1267...      0\n",
       " \n",
       " [134 rows x 2 columns])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop = True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('DataFrames/train_pfam_corpus_p4.csv', index = False)\n",
    "test.to_csv('DataFrames/test_pfam_corpus_p4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('DataFrames/train_pfam_corpus_p4.csv')\n",
    "df_test = pd.read_csv('DataFrames/test_pfam_corpus_p4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...      0\n",
       "1  PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...      1\n",
       "2  PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...      1\n",
       "3  PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...      1\n",
       "4  PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...      0"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF01370 PF0066...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF00583 PF00717 PF13508 PF13673 PF11799 PF0081...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF07728 PF00004 PF00654 PF00005 PF13561 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00072 PF00005 PF00501 PF13450 PF13193 PF0277...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00218 PF00291 PF00290 PF02146 PF01649 PF0251...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF00072 PF00501 PF00550 PF00890 PF01370 PF0066...      0\n",
       "1  PF00583 PF00717 PF13508 PF13673 PF11799 PF0081...      0\n",
       "2  PF07728 PF00004 PF00654 PF00005 PF13561 PF0010...      1\n",
       "3  PF00072 PF00005 PF00501 PF13450 PF13193 PF0277...      1\n",
       "4  PF00218 PF00291 PF00290 PF02146 PF01649 PF0251...      1"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, numpy.int64)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test.iloc[0].text), type(test.iloc[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore `tf-idf`\n",
    "- Each word in the corpus gets a value based on it's accurance and frequency\n",
    "- Each article (row) is an array of shape `(1, vocab_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 310\n"
     ]
    }
   ],
   "source": [
    "# articles_train is a 1D numpy  array of all train articels \n",
    "\n",
    "articles_train = df_train.text.values\n",
    "print( type(articles_train), len(articles_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PF02518 PF00512 PF00072 PF03176 PF04616 PF13561 PF00106 PF08659 PF00109 PF02801 PF00550 PF00890 PF13450 PF01593 PF13279 PF03061 PF13620 PF13715 PF13970 PF07977 PF03548 PF01522 PF04383 PF13723 PF08545 PF13489 PF00891 PF03279 PF01370 PF00221 PF09190 PF01406 PF09334 PF07523 PF07980 PF14322 PF00593 PF07715 PF10566 PF14508 PF12833 PF00165 PF07495 PF07494 PF00535 PF13641 PF01553 PF13704 PF09835 PF05175 PF13847 PF13649 PF08241 PF08242 PF05401 PF13589'"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000 characters of the 1st article in the train dataframe\n",
    "articles_train[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_test = df_test.text.values\n",
    "len(articles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 310\n"
     ]
    }
   ],
   "source": [
    "labels_train = df_train.label.values\n",
    "print( type(labels_train), len(labels_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 79)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Differenet rows has different length\n",
    "len(articles_train[0]), len(articles_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 134\n"
     ]
    }
   ],
   "source": [
    "labels_test = df_test.label.values\n",
    "print( type(labels_test), len(labels_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<444x1456 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12483 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = df_corpus.text.values\n",
    "X = tfidf_vectorizer.fit_transform(articles)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 1st article in dataset\n",
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 986)\t0.16538262692914568\n",
      "  (0, 383)\t0.20878944068577385\n",
      "  (0, 1003)\t0.2439856140402456\n",
      "  (0, 985)\t0.15414685198581074\n",
      "  (0, 1324)\t0.15414685198581074\n",
      "  (0, 1351)\t0.15904321057608348\n",
      "  (0, 820)\t0.16834609997876498\n",
      "  (0, 1051)\t0.17041646408818245\n",
      "  (0, 372)\t0.1461853719107809\n",
      "  (0, 1034)\t0.20268977007614375\n",
      "  (0, 337)\t0.20268977007614375\n",
      "  (0, 1029)\t0.20268977007614375\n",
      "  (0, 61)\t0.1526089522065723\n",
      "  (0, 329)\t0.179600012354591\n",
      "  (0, 653)\t0.15038148273564433\n",
      "  (0, 223)\t0.14229286920759293\n",
      "  (0, 1275)\t0.15572999054090678\n",
      "  (0, 1013)\t0.16538262692914568\n",
      "  (0, 1337)\t0.1518561730767219\n",
      "  (0, 769)\t0.22079965820477704\n",
      "  (0, 365)\t0.15819558942978412\n",
      "  (0, 684)\t0.15653937166399337\n",
      "  (0, 967)\t0.15414685198581074\n",
      "  (0, 1356)\t0.23999691322314187\n",
      "  (0, 635)\t0.1448563456110011\n",
      "  (0, 1208)\t0.14686239757223873\n",
      "  (0, 384)\t0.1549326006912145\n",
      "  (0, 1260)\t0.12131058829312995\n",
      "  (0, 222)\t0.13471609191396322\n",
      "  (0, 143)\t0.10314019107391009\n",
      "  (0, 594)\t0.11417655210815271\n",
      "  (0, 22)\t0.11343924153147902\n",
      "  (0, 134)\t0.10561524002521323\n",
      "  (0, 1016)\t0.11683970883881266\n",
      "  (0, 19)\t0.11568218400248705\n",
      "  (0, 1300)\t0.11683970883881266\n",
      "  (0, 135)\t0.16834609997876498\n",
      "  (0, 522)\t0.1607795814920677\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X` is a sparse matrix which is used for representating matrices with many 0 values\n",
    " - 444 articles\n",
    " - 1456 distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 23)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row has different length\n",
    "\n",
    "len(X[0].data), len(X[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16538263, 0.20878944, 0.24398561, 0.15414685, 0.15414685,\n",
       "       0.15904321, 0.1683461 , 0.17041646, 0.14618537, 0.20268977,\n",
       "       0.20268977, 0.20268977, 0.15260895, 0.17960001, 0.15038148,\n",
       "       0.14229287, 0.15572999, 0.16538263, 0.15185617, 0.22079966,\n",
       "       0.15819559, 0.15653937, 0.15414685, 0.23999691, 0.14485635,\n",
       "       0.1468624 , 0.1549326 , 0.12131059, 0.13471609, 0.10314019,\n",
       "       0.11417655, 0.11343924, 0.10561524, 0.11683971, 0.11568218,\n",
       "       0.11683971, 0.1683461 , 0.16077958])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23680896, 0.2759931 , 0.30948595, 0.2759931 , 0.21545961,\n",
       "       0.19147179, 0.24895247, 0.2652108 , 0.22290817, 0.17118446,\n",
       "       0.20900738, 0.23171795, 0.2033161 , 0.17402763, 0.16353908,\n",
       "       0.1936197 , 0.20900738, 0.1370062 , 0.11045416, 0.11673832,\n",
       "       0.12192887, 0.18196675, 0.17863302])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "- X is a sparse matrix which each row has a differenet length\n",
    "- We convert X to dense matrix and then to a tensor so all rows have the same length of 1456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_torch = torch.from_numpy(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1820, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1182, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_torch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_torch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `X_torch` is  a tensor with lots of 0 values and is now ready for dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to fit `TfidfVectorizer` only on train data and then used this trained vectorizer to transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_pfam = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer_pfam = tfidf_vectorizer_pfam.fit(articles_train)\n",
    "tfidf_vectorizer_pfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_vectorizer_pfam)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- Because each row is not in the form of One String (article) we need to define our own `make_dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "class make_dataset(Dataset):\n",
    "    def __init__ (self, file_name, vectorizer):\n",
    "        \n",
    "        if type(file_name) == str:\n",
    "            df = pd.read_csv(Dataset)\n",
    "        else:\n",
    "            df = file_name\n",
    "            \n",
    "        x = df.text.values\n",
    "        y = df.label.values\n",
    "        \n",
    "        self.X = vectorizer.transform(x)\n",
    "            \n",
    "        self.X = torch.from_numpy(self.X.todense())\n",
    "        self.X = self.X.long()\n",
    "\n",
    "#         sc = StandardScaler()\n",
    "#         X = sc.fit_transform(x)\n",
    "        \n",
    "        #y = LabelEncoder().fit_transform(y)\n",
    "        self.y = torch.tensor(y, dtype = torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        return self.X[idx], self.y[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = make_dataset(df_train, tfidf_vectorizer_pfam)\n",
    "len(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set = make_dataset(df_test, tfidf_vectorizer_pfam)\n",
    "len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_set, batch_size=5, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testing_set, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1314]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# for testing_set we have batch_size = 2, so in each batch we have 2 examples\n",
    "\n",
    "data_iter = iter(trainloader)\n",
    "\n",
    "this_train_article, this_train_label = next(data_iter)\n",
    "print(this_train_article.shape, this_train_label.shape)\n",
    "# this_pfam.T, this_response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- We fitted the tf-idf vectorizer only on train data (`articles_train`). Because of this, number of unique words is 1314 as you can see in the 2nd dimension of dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1314]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# for testing_set we have batch_size = 2, so in each batch we have 2 examples\n",
    "\n",
    "this_test_article, this_test_label = next(iter(testloader))\n",
    "print(this_test_article.shape, this_test_label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- The output of `torch.utils.data.DataLoader()`has `batch_size` as the 1st elemenet. Alwyas check this because it's important when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 1314.\n",
    "        self.fc1 = nn.Linear(1314, 512) \n",
    "        self.fc2 = nn.Linear(512, 256) \n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc_out = nn.Linear(64, 1) \n",
    "        \n",
    "        #self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(1024)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(\"inputs\", inputs.shape)# = torch.Size([5, 1314])\n",
    "        \n",
    "        x = torch.relu(self.fc1(inputs))\n",
    "        #print(\"fc1\", x.shape)# = torch.Size([5, 256])\n",
    "        \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #print(\"fc2\", x.shape) #= torch.Size([5, 64])\n",
    "        \n",
    "        x = torch.relu(self.fc3(x))\n",
    "        #print(\"fc3\", x.shape)# \n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #print(\"dropout\", x.shape)# = torch.Size([5, 64])\n",
    "               \n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        #print(\"fc_out\", x.shape)# = torch.Size([5, 1])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (fc1): Linear(in_features=1314, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(model, iterator, optimizer, criterion):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for pfam, response in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pfam = pfam.long()\n",
    "        pfam= pfam.to(device) # batch_size must be the 1st dimension so we don't use .T\n",
    "        pfam = pfam.float()\n",
    "     \n",
    "        response = response.to(device)\n",
    "        \n",
    "        # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "        predictions = model(pfam).squeeze(1)\n",
    "                \n",
    "        loss = criterion(predictions, response)\n",
    "        \n",
    "        rounded_preds = torch.round(predictions)\n",
    "        correct = (rounded_preds == response).float()\n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.757 | Train Acc: 76.13% \n",
      "| Epoch: 02 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 03 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 04 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 05 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 06 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 07 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 08 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 09 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 10 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 11 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 12 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 13 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 14 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 15 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 16 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 17 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 18 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 19 | Train Loss: 0.693 | Train Acc: 76.13% \n",
      "| Epoch: 20 | Train Loss: 0.693 | Train Acc: 76.13% \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train_linear(model, trainloader, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for pfam, response in iterator:\n",
    "        \n",
    "            pfam= pfam.to(device) # batch_size must be the 1st dimension so we don't use .T\n",
    "            pfam = pfam.float() # the model expects tensor of float type \n",
    "          \n",
    "            response = response.to(device)\n",
    "\n",
    "            # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "            predictions = model(pfam).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, response)\n",
    "\n",
    "            rounded_preds = torch.round(predictions)\n",
    "            correct = (rounded_preds == response).float()\n",
    "\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 2: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 3: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 4: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 5: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 6: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 7: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 8: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 9: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 10: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 11: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 12: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 13: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 14: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 15: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train_linear(model, trainloader, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate_linear(model, testloader, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models\\linear_model_saved_weights.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 838.041\n",
      "Iteration  1, inertia 405.116\n",
      "Iteration  2, inertia 402.082\n",
      "Iteration  3, inertia 400.004\n",
      "Iteration  4, inertia 399.004\n",
      "Iteration  5, inertia 398.897\n",
      "Iteration  6, inertia 398.793\n",
      "Iteration  7, inertia 398.667\n",
      "Iteration  8, inertia 397.594\n",
      "Iteration  9, inertia 397.329\n",
      "Iteration 10, inertia 397.241\n",
      "Iteration 11, inertia 397.227\n",
      "Iteration 12, inertia 397.133\n",
      "Iteration 13, inertia 397.057\n",
      "Iteration 14, inertia 397.011\n",
      "Converged at iteration 14: center shift 0.000000e+00 within tolerance 6.504874e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=2, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([357,  87], dtype=int64))"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF00072 PF03176 PF04616 PF1356...      0\n",
       "1  PF09950 PF11681 PF11863 PF13262 PF09979 PF0423...      1\n",
       "2  PF02518 PF00512 PF00072 PF04616 PF13561 PF0010...      1\n",
       "3  PF00072 PF00501 PF13450 PF12833 PF00165 PF1319...      1\n",
       "4  PF07690 PF13302 PF00926 PF07992 PF02535 PF0008...      0"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(r\"DataFrames\\train_pfam_corpus_p4.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    338\n",
       "1    106\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check for dupliactes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"pfam_vocab/train_pfam_corpus_p4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_finder(df, observation = 80):\n",
    "    from collections import Counter\n",
    "    \n",
    "    pfams2  = []\n",
    "    for txt in df[\"text\"]:\n",
    "        row = []\n",
    "        txt = txt.replace('.', '')\n",
    "        row.extend(word_tokenize(txt))\n",
    "        pfams2.append(row)\n",
    "\n",
    "    return Counter(pfams2[observation]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PF01555', 1),\n",
       " ('PF06406', 1),\n",
       " ('PF10784', 1),\n",
       " ('PF03515', 1),\n",
       " ('PF01024', 1),\n",
       " ('PF03857', 1),\n",
       " ('PF14859', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_finder(df_train, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PF13353', 1),\n",
       " ('PF00005', 1),\n",
       " ('PF00593', 1),\n",
       " ('PF07715', 1),\n",
       " ('PF00664', 1),\n",
       " ('PF13394', 1),\n",
       " ('PF13437', 1),\n",
       " ('PF03412', 1),\n",
       " ('PF13533', 1),\n",
       " ('PF12385', 1),\n",
       " ('PF01189', 1),\n",
       " ('PF13636', 1),\n",
       " ('PF17125', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_finder(df_corpus, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No duplicate found!\n",
    "- we're good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# operates on text and tokenize it using the tokenizer from nltk\n",
    "TEXT = torchtext.data.Field(tokenize = word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operates on labels and convert them to numeric values \n",
    "# in this case, Labels are already numeric so we don't need to do this conversion\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [(\"text\", TEXT), (\"labels\", LABEL)]\n",
    "# list of 2 tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = torchtext.data.TabularDataset.splits(path = \".\\DataFrames\", \n",
    "                                                train = \"train_pfam_corpus_p4.csv\",\n",
    "                                                test = \"test_pfam_corpus_p4.csv\" ,    \n",
    "                                                format = 'csv',\n",
    "                                                skip_header = True,\n",
    "                                                fields = datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x20522513988>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x20522513988>,\n",
       " <torchtext.data.example.Example at 0x20522513a08>,\n",
       " <torchtext.data.example.Example at 0x20522513a88>,\n",
       " <torchtext.data.example.Example at 0x20522513ac8>,\n",
       " <torchtext.data.example.Example at 0x205225139c8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 310\n",
      "Number of testing examples: 134\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trn)}')\n",
    "print(f'Number of testing examples: {len(tst)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always perfom this check to make sure that label and text are correct \n",
    "- In the dataset:\n",
    "\n",
    "> 1st is `text`\n",
    "\n",
    "> 2nd is `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PF13165',\n",
       " 'PF01842',\n",
       " 'PF07733',\n",
       " 'PF02811',\n",
       " 'PF14579',\n",
       " 'PF13484',\n",
       " 'PF03313',\n",
       " 'PF08331',\n",
       " 'PF03315',\n",
       " 'PF01955']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[5].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['PF09950', 'PF11681', 'PF11863', 'PF13262', 'PF09979', 'PF04233', 'PF14206', 'PF02498', 'PF09639', 'PF03374'], 'labels': '1'}\n"
     ]
    }
   ],
   "source": [
    "# a dict shape object which key is the label and value is a list of tokenized words\n",
    "print(vars(trn.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab creates a vocab of the input data in this case, training data\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 1316\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The additional 2 words in `TEXT.vocab` are:\n",
    "- `pad` : to fill empty spaces for each example\n",
    "- `unk`: to show words which are not present in train vocab but are in test vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PF00550', 120),\n",
       " ('PF00501', 119),\n",
       " ('PF00109', 98),\n",
       " ('PF02801', 96),\n",
       " ('PF00106', 93),\n",
       " ('PF13561', 91),\n",
       " ('PF08659', 91),\n",
       " ('PF04055', 87),\n",
       " ('PF13450', 83),\n",
       " ('PF00005', 77),\n",
       " ('PF13353', 77),\n",
       " ('PF00072', 76),\n",
       " ('PF03176', 74),\n",
       " ('PF13165', 70),\n",
       " ('PF00890', 69),\n",
       " ('PF00891', 56),\n",
       " ('PF07992', 55),\n",
       " ('PF03061', 54),\n",
       " ('PF01553', 54),\n",
       " ('PF13394', 53)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common 20 words in vocab with their frequencies\n",
    "TEXT.vocab.freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- Now data is in form ready for NLP (each row is One String) o we can use PyTorch predefined dataiterator `torchtext.data.BucketIterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (trn, tst),\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.text), # sort based on the length of email text \n",
    "    sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #self.model = model.cuda()\n",
    "        \n",
    "        # Creates dense embeding for each word instead of one-hot repesentation\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # hidden_dim is the output of the previous state of LSTM or RNN\n",
    "        # we feed one word at a time to the LSTM layer\n",
    "        # (1st word at time-step t, 2nd word at t+1, ...)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # input is the last hidden state of the LSTM and output is the prediction\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # text: (sentence_length, batch_size)\n",
    "        # Every input sentence is a list of indices of tf-idf encoded words\n",
    "        embedded = self.embedding(text)\n",
    "        # after applying embedded layer, every word in each sentence represented with its embedding\n",
    "        # embedded: (sentence_length, batch_size, embedding_dim)\n",
    "        \n",
    "        # to prevent overfitting\n",
    "        embedded_dropout = self.dropout(embedded)\n",
    "        \n",
    "        # the ouput batch of the embedding layer is the input to the RNN layer\n",
    "        # the preprocessing ensures that all sentences in each batch has the same length\n",
    "        output, (hidden, cell) = self.rnn(embedded_dropout)\n",
    "        # output: (sentence_length, batch_size, hiden_dim)\n",
    "        # output has all the hidden states for all words in a sentence\n",
    "        # hidden: (1, batch_size, hiden_dim), the last hidden state for each sentence\n",
    "        # cell is the output of last cell state for LSTM (Long Term Memory)\n",
    "        \n",
    "        # get rid of unnecessary dimension 1\n",
    "        hidden_1D = hidden.squeeze(0)\n",
    "        # hidden_1D: (batch_size, hiden_dim) only the last hidden state\n",
    "        \n",
    "        # to make sure that the output of the last hidden state is equal to hidden_1D\n",
    "        assert torch.equal(output[-1, :, :], hidden_1D)\n",
    "        \n",
    "        return self.fc(hidden_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab) # this network accepts one-hot encoded words 15002\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "output_dim = 1 # binary classification has only 1 neuron in the last layer\n",
    "\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-6)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch.text = batch.text.to(device)\n",
    "        batch.labels = batch.labels.to(device)\n",
    "        \n",
    "        # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "                \n",
    "        loss = criterion(predictions, batch.labels)\n",
    "        \n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.labels).float()\n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.682 | Train Acc: 68.84% \n",
      "| Epoch: 02 | Train Loss: 0.680 | Train Acc: 70.69% \n",
      "| Epoch: 03 | Train Loss: 0.682 | Train Acc: 67.53% \n",
      "| Epoch: 04 | Train Loss: 0.677 | Train Acc: 71.77% \n",
      "| Epoch: 05 | Train Loss: 0.676 | Train Acc: 75.16% \n",
      "| Epoch: 06 | Train Loss: 0.676 | Train Acc: 74.99% \n",
      "| Epoch: 07 | Train Loss: 0.676 | Train Acc: 73.88% \n",
      "| Epoch: 08 | Train Loss: 0.677 | Train Acc: 73.19% \n",
      "| Epoch: 09 | Train Loss: 0.673 | Train Acc: 75.09% \n",
      "| Epoch: 10 | Train Loss: 0.675 | Train Acc: 73.37% \n",
      "| Epoch: 11 | Train Loss: 0.671 | Train Acc: 73.33% \n",
      "| Epoch: 12 | Train Loss: 0.671 | Train Acc: 75.47% \n",
      "| Epoch: 13 | Train Loss: 0.669 | Train Acc: 75.36% \n",
      "| Epoch: 14 | Train Loss: 0.672 | Train Acc: 73.45% \n",
      "| Epoch: 15 | Train Loss: 0.666 | Train Acc: 74.90% \n",
      "| Epoch: 16 | Train Loss: 0.668 | Train Acc: 75.44% \n",
      "| Epoch: 17 | Train Loss: 0.666 | Train Acc: 75.56% \n",
      "| Epoch: 18 | Train Loss: 0.666 | Train Acc: 76.44% \n",
      "| Epoch: 19 | Train Loss: 0.664 | Train Acc: 75.73% \n",
      "| Epoch: 20 | Train Loss: 0.663 | Train Acc: 74.76% \n",
      "| Epoch: 21 | Train Loss: 0.664 | Train Acc: 76.06% \n",
      "| Epoch: 22 | Train Loss: 0.663 | Train Acc: 74.04% \n",
      "| Epoch: 23 | Train Loss: 0.660 | Train Acc: 75.98% \n",
      "| Epoch: 24 | Train Loss: 0.660 | Train Acc: 75.13% \n",
      "| Epoch: 25 | Train Loss: 0.661 | Train Acc: 76.24% \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    \n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for pfam, response in iterator:\n",
    "        \n",
    "            pfam= pfam.to(device) # batch_size must be the 2nd dimension so we use .T\n",
    "            pfam = pfam.long()\n",
    "            # torch.transpose(pfam, 0, 1) it works like transpose\n",
    "            response = response.to(device)\n",
    "\n",
    "            # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "            predictions = model(pfam).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, response)\n",
    "\n",
    "            rounded_preds = torch.round(predictions)\n",
    "            correct = (rounded_preds == response).float()\n",
    "\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.658 | Train Acc: 75.13%\n",
      "\t Loss: 0.647 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 2: Train Loss: 0.657 | Train Acc: 75.64%\n",
      "\t Loss: 0.646 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 3: Train Loss: 0.655 | Train Acc: 75.87%\n",
      "\t Loss: 0.645 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 4: Train Loss: 0.653 | Train Acc: 75.38%\n",
      "\t Loss: 0.644 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 5: Train Loss: 0.651 | Train Acc: 75.53%\n",
      "\t Loss: 0.643 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 6: Train Loss: 0.653 | Train Acc: 74.84%\n",
      "\t Loss: 0.642 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 7: Train Loss: 0.651 | Train Acc: 75.56%\n",
      "\t Loss: 0.641 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 8: Train Loss: 0.652 | Train Acc: 74.50%\n",
      "\t Loss: 0.640 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 9: Train Loss: 0.648 | Train Acc: 76.61%\n",
      "\t Loss: 0.639 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 10: Train Loss: 0.650 | Train Acc: 75.01%\n",
      "\t Loss: 0.638 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 11: Train Loss: 0.648 | Train Acc: 75.01%\n",
      "\t Loss: 0.637 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 12: Train Loss: 0.650 | Train Acc: 74.79%\n",
      "\t Loss: 0.636 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 13: Train Loss: 0.645 | Train Acc: 75.98%\n",
      "\t Loss: 0.635 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 14: Train Loss: 0.647 | Train Acc: 75.32%\n",
      "\t Loss: 0.634 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n",
      "Epoch 15: Train Loss: 0.644 | Train Acc: 75.81%\n",
      "\t Loss: 0.633 |  Val. Acc: 83.33%\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/proj3_model.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
