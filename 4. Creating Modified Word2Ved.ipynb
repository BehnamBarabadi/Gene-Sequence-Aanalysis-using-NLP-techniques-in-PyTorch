{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: **Implementing Word2Vec**\n",
    "- Creating embedding layer weights to be used in the RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PF02518 PF00512 PF13561 PF00106 PF08659 PF0050...      0\n",
       "1  PF07728 PF00004 PF13165 PF13353 PF04055 PF1339...      0\n",
       "2  PF00072 PF00501 PF00550 PF00890 PF13450 PF0137...      0\n",
       "3  PF00005 PF13561 PF00106 PF08659 PF00501 PF0010...      1\n",
       "4  PF00440 PF08541 PF01551 PF01613 PF03050 PF0162...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.read_csv(\"DataFrames/df_corpus_pfams.csv\")\n",
    "df_corpus.head()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 303)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_corpus.text[0]), len(df_corpus.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each row of `df_corpus` is One Big string consists of words\n",
    "- It's not a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of occurance of each element in a given row\n",
    "def duplicate_finder(df, row_number):\n",
    "    from collections import Counter\n",
    "\n",
    "    pfams2  = []\n",
    "    for txt in df[\"text\"]:\n",
    "        row = []\n",
    "        row.extend(word_tokenize(txt))\n",
    "        pfams2.append(row)\n",
    "\n",
    "    return Counter(pfams2[row_number]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PF00072', 1),\n",
       " ('PF00005', 1),\n",
       " ('PF00501', 1),\n",
       " ('PF13450', 1),\n",
       " ('PF13193', 1),\n",
       " ('PF02775', 1),\n",
       " ('PF04397', 1),\n",
       " ('PF14501', 1),\n",
       " ('PF07992', 1),\n",
       " ('PF00070', 1),\n",
       " ('PF14691', 1),\n",
       " ('PF00682', 1),\n",
       " ('PF00037', 1),\n",
       " ('PF13738', 1),\n",
       " ('PF04647', 1),\n",
       " ('PF01195', 1),\n",
       " ('PF02559', 1),\n",
       " ('PF04851', 1),\n",
       " ('PF00270', 1),\n",
       " ('PF00271', 1),\n",
       " ('PF03461', 1),\n",
       " ('PF01558', 1),\n",
       " ('PF01855', 1),\n",
       " ('PF02776', 1),\n",
       " ('PF01063', 1),\n",
       " ('PF09992', 1),\n",
       " ('PF01095', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all numbers should be 1 otherwise, we have duplicates which is not correct!\n",
    "duplicate_finder(df_corpus, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(df, numbers_to_check = 3, max_rows = 100):\n",
    "    \n",
    "    print(f\"Checking {numbers_to_check} random rows:\")    \n",
    "    for i in range(numbers_to_check):\n",
    "            print()\n",
    "            row_number = np.random.randint(max_rows)\n",
    "            print(\"checking row number:\", row_number, end = \" \") \n",
    "            for j in range(len(duplicate_finder(df, row_number))):\n",
    "                if j%10 == 0:\n",
    "                    print(\".\", end = \" \" )\n",
    "                if duplicate_finder(df, row_number)[j][1] > 1:\n",
    "                    print(\"duplicate found\")\n",
    "                    break\n",
    "    print()\n",
    "    print()\n",
    "    print(\"No duplicate found\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 3 random rows:\n",
      "\n",
      "checking row number: 152 . . . . . . . \n",
      "checking row number: 164 . . . \n",
      "checking row number: 385 . . . . . \n",
      "\n",
      "No duplicate found\n"
     ]
    }
   ],
   "source": [
    "find_duplicates(df_corpus, numbers_to_check=3, max_rows=444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a `corpus` which is a list of lists\n",
    "- It should have 444 elements which each is a list of words in each article\n",
    "- Each row is called an artice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "corpus = []\n",
    "corpus = [word_tokenize(article) for article in df_corpus.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PF13165',\n",
       " 'PF13353',\n",
       " 'PF04055',\n",
       " 'PF00890',\n",
       " 'PF02502',\n",
       " 'PF13394',\n",
       " 'PF13186',\n",
       " 'PF00156',\n",
       " 'PF07992',\n",
       " 'PF14681',\n",
       " 'PF13616',\n",
       " 'PF00639',\n",
       " 'PF00849',\n",
       " 'PF01624',\n",
       " 'PF00488',\n",
       " 'PF05192',\n",
       " 'PF05190',\n",
       " 'PF05188',\n",
       " 'PF06133',\n",
       " 'PF01938',\n",
       " 'PF00919']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary is a list of all unique words in corpus\n",
    "- We have 1456 unique words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `word_counts` is a dictionary of all words and their frequencies \n",
    "- (number of occrance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency (number of occrance) for 'PF13394' is: 70\n"
     ]
    }
   ],
   "source": [
    "# test an example\n",
    "# print the numbr of occurance of an example\n",
    "print(f\"The frequency (number of occrance) for 'PF13394' is: {word_counts['PF13394']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the index of a given word\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "# show the actual word for a given index\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['PF13394']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PF13394'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[1398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check these 2 lengthes are equal\n",
    "assert len(word_to_index) == len(index_to_word), print(\"something's wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PF07993': 0,\n",
       " 'PF06397': 1,\n",
       " 'PF01032': 2,\n",
       " 'PF13604': 3,\n",
       " 'PF13384': 4,\n",
       " 'PF01434': 5,\n",
       " 'PF02837': 6,\n",
       " 'PF05594': 7,\n",
       " 'PF00588': 8,\n",
       " 'PF16347': 9,\n",
       " 'PF00122': 10,\n",
       " 'PF00772': 11,\n",
       " 'PF08843': 12,\n",
       " 'PF10423': 13,\n",
       " 'PF00216': 14,\n",
       " 'PF00441': 15,\n",
       " 'PF06874': 16,\n",
       " 'PF02588': 17,\n",
       " 'PF08659': 18,\n",
       " 'PF03061': 19}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 20 elemets in word_to_index\n",
    "dict(itertools.islice(word_to_index.items(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
    "#         if gain < self.min_percent_gain:\n",
    "#             return True\n",
    "        if min(self.loss_list) < 1:\n",
    "            return True\n",
    "        \n",
    "#         else:\n",
    "#             return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    \n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    \n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = torch.from_numpy(np.array(batch_target)).long()\n",
    "            tensor_context = torch.from_numpy(np.array(batch_context)).long()\n",
    "            tensor_negative = torch.from_numpy(np.array(batch_negative)).long()\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "\n",
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tuples of `(target, context)` and 8 negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to the context_tuple_list:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "There are 80277 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4 # window size of 4\n",
    "negative_samples = sample_negative(8) # cerates 8 random samples for negative sampling\n",
    "\n",
    "print(\"Adding to the context_tuple_list:\")\n",
    "for article in corpus:\n",
    "    for i, word in enumerate(article):\n",
    "        if i%5000 == 0 :\n",
    "            print(\".\", end = ' ')\n",
    "        \n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(article))\n",
    "        \n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:                \n",
    "                context_tuple_list.append((word, article[j], next(negative_samples)))\n",
    "print()                \n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load a list\n",
    "- `context_tuple_list` is a list of tuples of target, context and 8 negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"context_tuple_list.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(context_tuple_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"context_tuple_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "    context_tuple_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PF08659',\n",
       " 'PF13561',\n",
       " ['PF13279',\n",
       "  'PF03444',\n",
       "  'PF00037',\n",
       "  'PF13673',\n",
       "  'PF01068',\n",
       "  'PF02901',\n",
       "  'PF07486',\n",
       "  'PF02361'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tuple_list[20]\n",
    "# the 1st element is the target wird\n",
    "# the 2nd element is the context word\n",
    "# the 3rd element is a list of 8 negative samples (incorrect contex words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context) # element-wise multipicatipn(batch_size, emb)\n",
    "        emb_product = torch.sum(emb_product, dim=1) # sum of all elements in each row (1, batch_dize)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        \n",
    "        # torch.bmm: Performs a batch matrix-matrix product of matrices\n",
    "        # both must be 3-D tensors each containing the same number of matrices.\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set the `embedding_size = 200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=0.5)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        \n",
    "        target_tensor = target_tensor.to(device)\n",
    "        context_tensor = context_tensor.to(device)\n",
    "        negative_tensor = negative_tensor.to(device)\n",
    "        \n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the the calculated weights of the model `state_dict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"linear_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (embeddings_target): Embedding(1456, 200)\n",
      "  (embeddings_context): Embedding(1456, 200)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# re-create the model with the same parameters\n",
    "net_loaded = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "\n",
    "\n",
    "# load the savded state_dict\n",
    "net_loaded.load_state_dict(torch.load('linear_model.pkl'))\n",
    "\n",
    "print(net_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net_loaded.embeddings_target.to('cpu') # embeddings_target is a layer's name in net \n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PF07993'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 10th word in the 6th element (row) of the corpus\n",
    "corpus[6][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gives the embedding of any given tensor\n",
    "def get_embedding(example):  \n",
    "    word_idx = word_to_index[example]\n",
    "    tensor_i = torch.tensor([word_idx], dtype=torch.long)\n",
    "    emb = net_loaded.embeddings_target.to('cpu')\n",
    "    v_i = emb(tensor_i)\n",
    "    print(v_i.shape)\n",
    "    print(v_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PF00501'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = corpus[55][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "tensor([[ 0.7189, -0.2708,  0.7607, -0.4706,  0.2762,  0.1573, -1.1624,  2.3537,\n",
      "          0.5249, -1.9238,  0.9707,  0.6295, -0.0648, -1.4088,  0.0157, -0.1950,\n",
      "         -0.6292, -0.3629, -0.7835,  1.2085,  0.7593, -0.1814, -0.8467,  0.7955,\n",
      "         -0.7397, -0.5660, -0.3464, -1.8314, -0.1965, -0.0588,  0.0325, -0.0115,\n",
      "          0.0462, -0.6450, -1.6439,  0.1170,  0.4583, -0.6088, -0.8033,  0.0345,\n",
      "          0.1159, -0.7268, -0.9968,  1.1516, -0.4881, -0.9967, -0.0648,  0.2730,\n",
      "          0.4092,  0.6889, -0.3140,  0.6734,  2.0125, -0.2978,  0.5582,  1.1533,\n",
      "          0.1928,  0.2110,  0.7036, -0.7814, -1.3970, -0.4693, -0.3137,  0.0097,\n",
      "          1.0269,  0.7430, -0.6164, -0.7070,  1.3978,  0.6560,  0.4312,  0.9657,\n",
      "          1.4866,  1.1634, -1.3779, -0.4978, -0.3533,  0.6932,  1.4032, -1.6677,\n",
      "         -1.1841, -0.3967,  1.1742, -0.0981, -0.0533,  0.0421,  1.3512,  1.0210,\n",
      "          0.8692, -0.3518, -1.5903, -1.0461, -1.2040, -0.9503,  0.0405,  0.2029,\n",
      "          0.0229,  0.2086, -0.5431, -0.4206, -0.7540, -0.7258, -0.8575,  1.2353,\n",
      "          2.1707, -0.3368,  0.3168,  0.5436,  1.3841,  1.0992, -2.4791,  0.4122,\n",
      "         -1.3416, -1.0995,  0.6987, -0.2454,  0.0779, -0.6394, -1.4262, -0.3996,\n",
      "         -0.4149, -0.4445,  0.1449, -1.2052,  0.0732,  0.3537, -0.3135, -0.7406,\n",
      "         -0.5208, -0.2608, -0.1385, -1.8490,  0.1292,  0.5348, -0.2747, -0.2061,\n",
      "         -0.2188,  1.9041,  0.2242,  1.9077, -0.6620, -0.9598, -1.4632, -0.5117,\n",
      "          1.5460,  1.8761, -0.0627, -1.3596,  1.0219, -1.2000, -0.4421,  2.1730,\n",
      "         -0.4309, -0.1429, -0.5635, -0.3856,  1.3146,  1.1089, -1.0251, -0.4267,\n",
      "         -0.9436,  0.1043, -0.8737, -0.5257, -1.5925,  1.3619,  0.3327, -0.6017,\n",
      "         -0.8175, -0.4693, -1.1852,  0.1969,  1.3397, -1.6790,  1.5664, -0.1491,\n",
      "          1.5877,  0.8270, -0.9654,  0.2781,  0.4780,  0.4962,  1.5825, -0.8108,\n",
      "         -0.2742, -1.0367,  0.1531,  0.1932,  0.6972,  0.1394, -0.2037, -0.5449,\n",
      "         -0.8394,  0.2981,  1.0297,  0.8650, -0.4609, -1.1321,  1.1749, -0.4559]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "get_embedding('PF00501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1456, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_loaded.embeddings_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PF06397'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5359, -0.7684,  0.7282, -0.7458,  0.2250,  0.8369, -0.1053,  0.6144,\n",
       "         -1.5434,  1.3837, -0.0328,  0.8713,  1.0123,  1.0426, -1.1505, -0.0632,\n",
       "         -0.1059, -0.7121,  0.0374, -0.4777, -0.5480,  1.1887, -0.1980,  2.3879,\n",
       "         -0.0540, -0.2098, -1.9747,  0.5113,  0.0321,  0.3041, -0.2379, -1.8359,\n",
       "          0.9093,  0.5847, -2.1586, -1.2363,  0.9626, -1.5322,  0.1414, -1.0482,\n",
       "         -0.1414, -1.6646,  0.0410,  0.2592,  0.3366, -0.2584,  1.6519, -0.3203,\n",
       "         -0.8890,  1.0372,  1.7368, -0.4734,  0.5459,  0.5644, -0.5998, -0.8241,\n",
       "          0.2712,  0.2275, -0.8104,  0.4923, -0.3675, -0.7963, -0.3968,  0.7664,\n",
       "         -0.1714, -0.2046,  0.8938, -0.4721,  1.4047, -1.3800,  1.3890, -0.0399,\n",
       "          0.0359,  0.8458,  0.5668, -1.0664,  0.6646,  1.3218,  0.1220, -1.8032,\n",
       "          0.9963, -1.0555,  1.2964, -0.7797,  0.5181, -0.4212, -0.0137, -1.5138,\n",
       "          0.6439,  1.1263, -0.2956,  1.1352,  0.6468,  0.1048,  0.5438,  0.3626,\n",
       "         -0.9292, -0.4250,  0.9054,  0.2873,  1.3478, -0.1497, -0.4263, -1.0910,\n",
       "          0.1969,  0.5877, -0.1013, -0.2576,  0.5779,  0.0146,  0.2302, -1.0704,\n",
       "          0.3609,  0.0294, -0.8804,  1.0751, -2.3327, -1.1327,  0.4824,  0.5311,\n",
       "         -1.1885,  0.7521,  0.5510, -0.0041, -0.5233, -0.2116,  0.2872,  0.4141,\n",
       "          1.2262,  2.1639,  0.9492, -0.7158,  0.2903, -0.9776,  2.5009,  1.3185,\n",
       "          3.2147, -0.3771,  1.4211,  0.2763,  0.6047,  0.3523, -1.2388,  0.5140,\n",
       "          1.7997,  0.1061, -1.1428,  1.9654, -1.6739, -0.1667,  0.2531,  1.7313,\n",
       "          0.5371,  0.9501,  0.5422, -0.3633,  1.1366,  0.1988, -0.1473,  0.0761,\n",
       "         -0.7745, -1.3347, -1.1233, -0.1124, -1.2269,  0.0992,  1.7585,  1.1351,\n",
       "          0.8164, -0.7359,  0.7138,  0.5535,  2.0235, -0.5346,  0.3014,  1.9161,\n",
       "          0.1515,  0.3898, -0.2013,  1.0764,  2.1901,  1.0872, -0.2881,  0.3892,\n",
       "         -0.4783, -1.5693, -0.1723,  0.3148, -0.6812, -2.0274,  1.3760, -1.0103,\n",
       "         -0.2912, -1.6950, -1.7907,  0.7983, -1.4521, -0.7704,  0.4274, -0.0890]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Get embeddings for the word at index 1 which is 'PF13742'\n",
    "given_tensor = torch.LongTensor([1]) \n",
    "\n",
    "emb = net_loaded.embeddings_target.to('cpu')\n",
    "emb(given_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1012517 , -0.73667127, -0.88576293, ..., -0.28950164,\n",
       "        -1.2180898 , -1.6013262 ],\n",
       "       [ 0.5359053 , -0.7683729 ,  0.72822714, ..., -0.77044475,\n",
       "         0.42743716, -0.08898509],\n",
       "       [ 0.12467647, -0.25790042,  0.16013083, ..., -0.21123819,\n",
       "        -2.0667732 , -1.626157  ],\n",
       "       ...,\n",
       "       [-0.60910374,  0.6216965 , -0.8842806 , ..., -0.01349114,\n",
       "         0.7565897 , -0.4376295 ],\n",
       "       [ 0.02750395, -1.6208653 , -0.01751652, ..., -0.21836942,\n",
       "        -0.3727686 , -0.03830582],\n",
       "       [-0.3886597 , -0.9766208 , -0.46216154, ..., -0.43965217,\n",
       "        -0.79844314,  0.03470441]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m.weight contains the embedding weights.\n",
    "embed_weights = net_loaded.embeddings_target.weight.detach().numpy()\n",
    "embed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_dim\n",
    "len(embed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed_dim\n",
    "len(embed_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1456 embeddings which is equal to the total number of pfams\n",
      "and each embedding has 200 values (a tensor of size (1x 200)) which is equal to the embedding_dim we set before.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"We have {len(embed_weights)} embeddings which is equal to the total number of rows  \n",
    "and each embedding has 200 values (a tensor of size (1x 200)) which is equal to the embedding_dim we set before.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embed_weights.txt', embed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10125172, -0.73667127, -0.88576293, ..., -0.28950164,\n",
       "        -1.21808982, -1.60132623],\n",
       "       [ 0.5359053 , -0.76837289,  0.72822714, ..., -0.77044475,\n",
       "         0.42743716, -0.08898509],\n",
       "       [ 0.12467647, -0.25790042,  0.16013083, ..., -0.21123819,\n",
       "        -2.06677318, -1.62615705],\n",
       "       ...,\n",
       "       [-0.60910374,  0.62169647, -0.88428062, ..., -0.01349114,\n",
       "         0.75658971, -0.43762949],\n",
       "       [ 0.02750395, -1.62086535, -0.01751652, ..., -0.21836942,\n",
       "        -0.37276861, -0.03830582],\n",
       "       [-0.38865969, -0.97662079, -0.46216154, ..., -0.43965217,\n",
       "        -0.79844314,  0.03470441]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights = np.loadtxt('embed_weights.txt')\n",
    "embed_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the embedding weights for a given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9294, -0.4915,  1.0318, -0.4365, -0.5282, -1.0763, -1.0245, -0.3804,\n",
       "         -0.1479, -1.5536,  1.7433, -1.4383, -0.4809,  1.1201,  1.1437,  0.0459,\n",
       "          0.3696, -2.1861, -0.5929,  1.8557, -1.2914,  1.4966,  0.8013, -0.5334,\n",
       "         -0.9920, -0.0042,  0.1584,  0.4800, -0.6065, -0.1880, -0.1205, -0.3557,\n",
       "          0.8604,  0.1648,  1.1942,  0.5635, -0.1705,  0.9610, -1.2785, -0.4699,\n",
       "          1.1572,  2.0063, -0.3878,  0.6736, -0.4867, -1.7404, -0.5976,  0.0538,\n",
       "          0.3461,  0.8936, -0.4225, -0.2287, -0.6770, -1.5320,  1.0000,  0.8466,\n",
       "         -0.8438, -0.1719,  0.8197,  0.7720, -0.6393, -0.7313,  0.0736,  0.9093,\n",
       "         -1.1983, -0.6063,  0.8366, -0.1000,  0.4303,  0.1646,  0.6053,  0.2938,\n",
       "          0.7185,  1.8824,  0.7304, -0.1334, -0.5072,  0.1319, -0.4822, -0.5414,\n",
       "          1.6454,  0.5448, -0.8725,  0.7131, -0.2712,  0.7965,  0.9743,  1.2016,\n",
       "          0.3105,  1.3030, -0.8468,  0.2280, -0.8793,  1.2977, -0.1493, -0.5487,\n",
       "         -1.7064, -0.1393, -0.8297,  0.8823,  0.0996, -1.1648, -1.0262,  1.4744,\n",
       "         -0.7296,  0.7515,  0.2181,  1.9545, -1.6134, -2.4338, -1.1969, -0.0820,\n",
       "          0.6176,  0.4420, -0.2443, -0.3636, -0.0049,  0.2255, -0.7349, -2.0420,\n",
       "          2.0489,  0.0842,  0.3446, -0.2316,  1.9618, -1.3258,  1.1788,  0.4445,\n",
       "          1.3357,  0.2391, -0.0084, -0.7052,  0.0767, -0.6588,  2.0720,  1.1342,\n",
       "         -0.4002, -1.0977, -0.9957,  3.0811,  1.7309,  0.3945, -0.6866, -0.1059,\n",
       "         -0.9703,  1.1471, -0.8669, -0.4770, -0.0947,  0.8515, -0.9751, -0.5838,\n",
       "          0.5140, -0.9207, -0.0400,  0.0306, -0.3037, -0.7160, -0.1283,  0.9358,\n",
       "         -0.4803, -0.1703, -0.8140, -0.2823,  0.2677, -0.1317,  0.1781,  1.0207,\n",
       "         -0.1379, -0.3075, -0.2434,  0.6290,  1.0945,  0.2442, -3.0883,  0.0359,\n",
       "          1.7636, -0.1863,  0.6940, -0.3617,  1.2663, -1.6220,  2.4453,  0.7834,\n",
       "         -0.8234, -1.6889, -0.3909, -0.0991,  1.2742,  0.7173, -0.1222,  2.3298,\n",
       "          0.3493,  0.1464,  2.0148, -1.3552,  1.2745, -1.1102, -0.2088, -0.4990]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocabulary_size = len(vocabulary) # 1456\n",
    "\n",
    "# FloatTensor containing pretrained weights\n",
    "tensor_weight = torch.FloatTensor(embed_weights)\n",
    "embedding = nn.Embedding(vocabulary_size, 200).from_pretrained(tensor_weight)\n",
    "\n",
    "# Get embeddings for index 100\n",
    "given_tensor = torch.LongTensor([100])\n",
    "embedding(given_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use these weights in the embedding layer of our model in the next project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
