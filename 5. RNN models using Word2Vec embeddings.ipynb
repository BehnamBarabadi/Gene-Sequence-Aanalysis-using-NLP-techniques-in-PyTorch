{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: **Building RNN Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_12424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_46673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034069_k141_61665</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034070_k141_24121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034071_k141_39888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Response                    BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "0  NonResponder  ERS2034069_k141_12424        0        0        0        0   \n",
       "1  NonResponder  ERS2034069_k141_46673        0        0        0        0   \n",
       "2  NonResponder  ERS2034069_k141_61665        0        0        0        0   \n",
       "3  NonResponder  ERS2034070_k141_24121        0        0        0        0   \n",
       "4  NonResponder  ERS2034071_k141_39888        0        0        0        0   \n",
       "\n",
       "   PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "0        0        0        0        0  ...        0        0        0   \n",
       "1        0        0        0        0  ...        0        0        0   \n",
       "2        0        0        0        0  ...        0        0        0   \n",
       "3        0        0        0        0  ...        0        0        0   \n",
       "4        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "   PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "0        0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DataFrames/final_pfam_dataframe_no_ID.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state = 42, stratify = df.Response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train shape: (310, 1458)\n",
      "  test shape: (134, 1458)\n"
     ]
    }
   ],
   "source": [
    "print(f\" train shape: {train.shape}\\n  test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       " 0    NonResponder   ERS2034085_k141_32666        0        0        0        0   \n",
       " 1       Responder   ERS2034277_k141_55826        0        0        0        0   \n",
       " 2       Responder   ERS2034146_k141_37159        0        0        0        0   \n",
       " 3       Responder    ERS2034167_k141_9000        0        0        0        0   \n",
       " 4    NonResponder  ERS2034159_k141_204702        0        0        0        0   \n",
       " ..            ...                     ...      ...      ...      ...      ...   \n",
       " 305  NonResponder  ERS2034268_k141_115367        0        0        0        0   \n",
       " 306  NonResponder   ERS2034275_k141_55871        0        0        0        0   \n",
       " 307     Responder   ERS2034192_k141_93958        0        0        0        0   \n",
       " 308  NonResponder  ERS2034218_k141_180976        0        0        0        0   \n",
       " 309     Responder    ERS2034116_k141_3854        0        0        0        0   \n",
       " \n",
       "      PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       " 0          0        0        0        0  ...        0        0        0   \n",
       " 1          0        0        0        0  ...        0        0        0   \n",
       " 2          0        0        0        0  ...        0        0        0   \n",
       " 3          0        0        0        0  ...        0        0        0   \n",
       " 4          0        0        0        0  ...        0        0        0   \n",
       " ..       ...      ...      ...      ...  ...      ...      ...      ...   \n",
       " 305        0        0        0        0  ...        0        0        0   \n",
       " 306        0        0        0        0  ...        0        0        0   \n",
       " 307        0        1        1        0  ...        0        0        0   \n",
       " 308        0        1        0        0  ...        0        0        0   \n",
       " 309        0        0        0        0  ...        0        0        0   \n",
       " \n",
       "      PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       " 0          0        0        0        0        0        0        0  \n",
       " 1          0        0        0        0        0        0        0  \n",
       " 2          0        0        0        0        0        0        0  \n",
       " 3          0        0        0        0        0        0        0  \n",
       " 4          0        0        0        0        0        0        0  \n",
       " ..       ...      ...      ...      ...      ...      ...      ...  \n",
       " 305        0        0        0        0        0        0        0  \n",
       " 306        0        0        0        0        0        0        0  \n",
       " 307        0        0        0        0        0        0        0  \n",
       " 308        0        0        0        0        0        0        0  \n",
       " 309        0        0        0        0        0        0        0  \n",
       " \n",
       " [310 rows x 1458 columns],\n",
       "          Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       " 0    NonResponder   ERS2034243_k141_57259        0        0        0        0   \n",
       " 1    NonResponder   ERS2034102_k141_19669        0        0        0        0   \n",
       " 2       Responder  ERS2034234_k141_216502        0        0        0        0   \n",
       " 3       Responder  ERS2034203_k141_186898        0        0        0        0   \n",
       " 4       Responder   ERS2034118_k141_81445        0        0        0        0   \n",
       " ..            ...                     ...      ...      ...      ...      ...   \n",
       " 129  NonResponder   ERS2034071_k141_39888        0        0        0        0   \n",
       " 130  NonResponder   ERS2034284_k141_76069        0        0        0        0   \n",
       " 131  NonResponder   ERS2034073_k141_82004        0        0        0        0   \n",
       " 132  NonResponder    ERS2034078_k141_7735        0        0        0        0   \n",
       " 133  NonResponder    ERS2034102_k141_9512        0        0        0        0   \n",
       " \n",
       "      PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       " 0          0        0        0        0  ...        0        0        0   \n",
       " 1          0        0        0        0  ...        0        0        0   \n",
       " 2          0        0        0        0  ...        0        0        0   \n",
       " 3          0        0        0        0  ...        0        0        0   \n",
       " 4          0        0        0        0  ...        0        0        0   \n",
       " ..       ...      ...      ...      ...  ...      ...      ...      ...   \n",
       " 129        0        0        0        0  ...        0        0        0   \n",
       " 130        0        0        0        0  ...        0        0        0   \n",
       " 131        0        0        0        0  ...        0        0        0   \n",
       " 132        0        0        0        0  ...        0        0        0   \n",
       " 133        0        0        0        0  ...        0        0        0   \n",
       " \n",
       "      PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       " 0          0        0        0        0        0        0        0  \n",
       " 1          0        0        0        0        0        0        0  \n",
       " 2          0        0        0        0        0        0        0  \n",
       " 3          0        0        0        0        0        0        0  \n",
       " 4          0        0        0        0        0        0        0  \n",
       " ..       ...      ...      ...      ...      ...      ...      ...  \n",
       " 129        0        0        0        0        0        0        0  \n",
       " 130        0        0        0        0        0        0        0  \n",
       " 131        0        0        0        0        0        0        0  \n",
       " 132        0        0        0        0        0        0        0  \n",
       " 133        0        0        0        0        0        0        0  \n",
       " \n",
       " [134 rows x 1458 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True) , test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034085_k141_32666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034277_k141_55826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034146_k141_37159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034167_k141_9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034159_k141_204702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "30   NonResponder   ERS2034085_k141_32666        0        0        0        0   \n",
       "420     Responder   ERS2034277_k141_55826        0        0        0        0   \n",
       "153     Responder   ERS2034146_k141_37159        0        0        0        0   \n",
       "209     Responder    ERS2034167_k141_9000        0        0        0        0   \n",
       "182  NonResponder  ERS2034159_k141_204702        0        0        0        0   \n",
       "\n",
       "     PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "30         0        0        0        0  ...        0        0        0   \n",
       "420        0        0        0        0  ...        0        0        0   \n",
       "153        0        0        0        0  ...        0        0        0   \n",
       "209        0        0        0        0  ...        0        0        0   \n",
       "182        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "     PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "30         0        0        0        0        0        0        0  \n",
       "420        0        0        0        0        0        0        0  \n",
       "153        0        0        0        0        0        0        0  \n",
       "209        0        0        0        0        0        0        0  \n",
       "182        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034243_k141_57259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034102_k141_19669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034234_k141_216502</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034203_k141_186898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034118_k141_81445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "370  NonResponder   ERS2034243_k141_57259        0        0        0        0   \n",
       "59   NonResponder   ERS2034102_k141_19669        0        0        0        0   \n",
       "355     Responder  ERS2034234_k141_216502        0        0        0        0   \n",
       "296     Responder  ERS2034203_k141_186898        0        0        0        0   \n",
       "95      Responder   ERS2034118_k141_81445        0        0        0        0   \n",
       "\n",
       "     PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "370        0        0        0        0  ...        0        0        0   \n",
       "59         0        0        0        0  ...        0        0        0   \n",
       "355        0        0        0        0  ...        0        0        0   \n",
       "296        0        0        0        0  ...        0        0        0   \n",
       "95         0        0        0        0  ...        0        0        0   \n",
       "\n",
       "     PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "370        0        0        0        0        0        0        0  \n",
       "59         0        0        0        0        0        0        0  \n",
       "355        0        0        0        0        0        0        0  \n",
       "296        0        0        0        0        0        0        0  \n",
       "95         0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Test  and Train in seperate csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('DataFrames/train_pfam_p3.csv', index=False )\n",
    "test.to_csv('DataFrames/test_pfam_p3.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is SSD\n",
      " Volume Serial Number is F6B3-93A4\n",
      "\n",
      " Directory of D:\\Google Drive\\Jupyter Notebooks\\VastBiome\\Routy_Data_Processed\n",
      "\n",
      "12/01/2020  02:29 PM    <DIR>          .\n",
      "12/01/2020  02:29 PM    <DIR>          ..\n",
      "12/01/2020  02:29 PM    <DIR>          .ipynb_checkpoints\n",
      "09/09/2020  05:18 PM    <DIR>          CLANS\n",
      "09/09/2020  12:24 PM        65,012,965 context_tuple_list.txt\n",
      "08/26/2020  06:28 PM    <DIR>          corpus_new\n",
      "10/20/2020  01:01 PM            79,769 corpus_new.rar\n",
      "08/19/2020  11:33 AM    <DIR>          DataFrames\n",
      "08/19/2020  11:26 AM        13,921,206 df_corpus_pfams.csv\n",
      "11/30/2020  10:53 PM               961 df_feature_importance.csv\n",
      "09/09/2020  11:03 AM         7,426,868 embed_weights.txt\n",
      "08/19/2020  09:59 AM         1,325,229 final_dataframe.csv\n",
      "08/19/2020  10:00 AM         1,320,342 final_dataframe_no_ID.csv\n",
      "09/09/2020  05:14 PM             1,033 find_duplicate.py\n",
      "08/09/2020  09:27 PM             2,391 linear_model\n",
      "08/27/2020  11:36 AM         2,330,248 linear_model.pkl\n",
      "09/17/2020  11:26 AM    <DIR>          models\n",
      "09/08/2020  05:39 PM    <DIR>          Pathways\n",
      "09/08/2020  10:50 AM    <DIR>          PFams\n",
      "08/10/2020  05:35 PM        21,789,692 pfam_corpus.txt\n",
      "08/13/2020  07:13 PM    <DIR>          pfam_vocab\n",
      "08/03/2020  05:53 PM    <DIR>          Routy_Data_Processed\n",
      "08/07/2020  05:27 PM    <DIR>          Splited\n",
      "08/11/2020  05:17 PM         3,332,083 test.plk\n",
      "08/08/2020  09:08 AM           879,090 test_pfam.csv\n",
      "08/13/2020  07:15 PM         3,880,925 test_pfam_corpus_p4.csv\n",
      "08/11/2020  06:20 PM           878,589 test_pfam_p3.csv\n",
      "08/08/2020  09:08 AM         2,000,422 train_pfam.csv\n",
      "08/13/2020  07:15 PM        10,040,293 train_pfam_corpus_p4.csv\n",
      "08/11/2020  06:20 PM         1,999,255 train_pfam_p3.csv\n",
      "10/19/2020  03:41 PM            66,533 VastBiome_1.ipynb\n",
      "09/17/2020  10:45 AM           108,852 VastBiome_2.ipynb\n",
      "11/24/2020  08:14 AM           101,531 VastBiome_3.ipynb\n",
      "11/15/2020  06:29 PM            85,822 VastBiome_3_Creating the Corpus, Tf-idf, Linear and RNN Model using BucketIterator.ipynb\n",
      "11/30/2020  10:53 PM           255,611 VastBiome_5 RNN Models.ipynb\n",
      "10/22/2020  06:26 PM            94,262 VastBiome_6_Topic Modeling.ipynb\n",
      "11/16/2020  09:15 AM               662 VastBiome_7_CNN_for_classificaton.ipynb\n",
      "09/17/2020  11:29 AM            40,349 VatBiome_4_Word2Vec.ipynb\n",
      "08/03/2020  05:53 PM    <DIR>          __MACOSX\n",
      "              26 File(s)    136,974,983 bytes\n",
      "              13 Dir(s)  878,287,233,024 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034085_k141_32666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034277_k141_55826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034146_k141_37159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034167_k141_9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034159_k141_204702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "0  NonResponder   ERS2034085_k141_32666        0        0        0        0   \n",
       "1     Responder   ERS2034277_k141_55826        0        0        0        0   \n",
       "2     Responder   ERS2034146_k141_37159        0        0        0        0   \n",
       "3     Responder    ERS2034167_k141_9000        0        0        0        0   \n",
       "4  NonResponder  ERS2034159_k141_204702        0        0        0        0   \n",
       "\n",
       "   PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "0        0        0        0        0  ...        0        0        0   \n",
       "1        0        0        0        0  ...        0        0        0   \n",
       "2        0        0        0        0  ...        0        0        0   \n",
       "3        0        0        0        0  ...        0        0        0   \n",
       "4        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "   PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "0        0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('DataFrames/train_pfam_p3.csv')\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034243_k141_57259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034102_k141_19669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034234_k141_216502</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034203_k141_186898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034118_k141_81445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "0  NonResponder   ERS2034243_k141_57259        0        0        0        0   \n",
       "1  NonResponder   ERS2034102_k141_19669        0        0        0        0   \n",
       "2     Responder  ERS2034234_k141_216502        0        0        0        0   \n",
       "3     Responder  ERS2034203_k141_186898        0        0        0        0   \n",
       "4     Responder   ERS2034118_k141_81445        0        0        0        0   \n",
       "\n",
       "   PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "0        0        0        0        0  ...        0        0        0   \n",
       "1        0        0        0        0  ...        0        0        0   \n",
       "2        0        0        0        0  ...        0        0        0   \n",
       "3        0        0        0        0  ...        0        0        0   \n",
       "4        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "   PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "0        0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('DataFrames/test_pfam_p3.csv')\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "class make_dataset(Dataset):\n",
    "    def __init__ (self, file_name):\n",
    "        \n",
    "        df = pd.read_csv(file_name)\n",
    "        x = df.iloc[:, 2:].values\n",
    "        y = df.iloc[:, 0].values\n",
    "\n",
    "        self.X = torch.tensor(x, dtype = torch.long) # use torch.long because value are integers\n",
    "#         sc = StandardScaler()\n",
    "#         X = sc.fit_transform(x)\n",
    "        \n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        self.y = torch.tensor(y, dtype = torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        return self.X[idx], self.y[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = make_dataset('DataFrames/train_pfam_p3.csv')\n",
    "testing_set = make_dataset('DataFrames/test_pfam_p3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test an examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>BGC</th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034085_k141_32666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034277_k141_55826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034146_k141_37159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Responder</td>\n",
       "      <td>ERS2034167_k141_9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>ERS2034159_k141_204702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Response                     BGC  PF00218  PF00291  PF00290  PF02146  \\\n",
       "30   NonResponder   ERS2034085_k141_32666        0        0        0        0   \n",
       "420     Responder   ERS2034277_k141_55826        0        0        0        0   \n",
       "153     Responder   ERS2034146_k141_37159        0        0        0        0   \n",
       "209     Responder    ERS2034167_k141_9000        0        0        0        0   \n",
       "182  NonResponder  ERS2034159_k141_204702        0        0        0        0   \n",
       "\n",
       "     PF01649  PF02518  PF00512  PF00486  ...  PF15902  PF09822  PF03739  \\\n",
       "30         0        0        0        0  ...        0        0        0   \n",
       "420        0        0        0        0  ...        0        0        0   \n",
       "153        0        0        0        0  ...        0        0        0   \n",
       "209        0        0        0        0  ...        0        0        0   \n",
       "182        0        0        0        0  ...        0        0        0   \n",
       "\n",
       "     PF13690  PF04509  PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "30         0        0        0        0        0        0        0  \n",
       "420        0        0        0        0        0        0        0  \n",
       "153        0        0        0        0        0        0        0  \n",
       "209        0        0        0        0        0        0        0  \n",
       "182        0        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 1458 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor([0, 0, 0,  ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[4][1], training_set[4][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 134)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set), len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_set, batch_size=5, shuffle=True,)\n",
    "testloader = torch.utils.data.DataLoader(testing_set, batch_size=2, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1456]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(trainloader)\n",
    "\n",
    "this_pfam, this_response = next(data_iter)\n",
    "print(this_pfam.shape, this_response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1456]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# for testing_set we have batch_size = 2, so in each batch we have 2 examples\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "\n",
    "this_pfam, this_response = next(data_iter)\n",
    "print(this_pfam.shape, this_response.shape)\n",
    "# this_pfam.T, this_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n",
      "torch.Size([2, 1456]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for pfam, response in testloader:\n",
    "    print(pfam.shape, response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n",
      "torch.Size([5, 1456]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for pfam, response in trainloader:\n",
    "    print(pfam.shape, response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1456])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dim = pfam.shape[1]\n",
    "vocab_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention!\n",
    "- We are using pretrained embedding wights `embed_weights` in this model\n",
    "- These weights are calculatd in `VatBiome_5_Word2Vec.ipynb` and saved in `'embed_weights.txt'` file\n",
    "- These weights are calculated by using an embedding layer of :\n",
    "\n",
    "> `vocab_dim = 1456`\n",
    "\n",
    "> `embed_dim = 200`\n",
    "\n",
    "so when defining the RNN model in the following cells we have to set `embed_dim = 200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__ (self, input_dim, embed_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        embed_weights = np.loadtxt('embed_weights.txt') # load the pretrained weights\n",
    "        tensor_weight = torch.FloatTensor(embed_weights) # convert the pretrained weights into tensors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim).from_pretrained(tensor_weight)\n",
    "        # use pretrained weights in Embedding layer\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"x\", x.shape)# = (1456, 5)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        print(\"x\", x.shape)# = (1456, 5, 200)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(x)        \n",
    "        print(\"output\", output.shape)# = (1456, 5, 256)\n",
    "        print(\"hidden\", hidden.shape)# = (1, 5, 256)\n",
    "        \n",
    "        hidden_1D = hidden.squeeze()\n",
    "        print(\"hidden_1D\", hidden_1D.shape)# = (5, 256)\n",
    "            \n",
    "        assert torch.equal(output[-1], hidden_1D)\n",
    "        \n",
    "        # input to the fc layer is (batch_size, num_features)\n",
    "        x = torch.sigmoid(self.fc(hidden_1D))\n",
    "        print(\"last\", x.shape)# = (5, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = vocab_dim\n",
    "embed_dim = 200\n",
    "hidden_dim = 256\n",
    "output_dim = 1 # binary classification has only 1 neuron in the last layer\n",
    "\n",
    "vast_rnn = RNN(input_dim, embed_dim, hidden_dim, output_dim, n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vast_rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention!\n",
    "> The default setting for `LSTM`  cell is to give `batch_size` as the 2nd dimension to the model, on the other hand because we have used `Dataloader`, each batch of data is in `(batch_size, seq_length)` format so we need to use `.T` function to transpose `pfam` tensor to place the `batch_size` as the 2nd dimension. \n",
    "\n",
    "> If we want to input `pfam` as it is which is `(batch_size, seq_length)` we have to use `batch_first = True` when defining `self.lstm` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for pfam, response in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pfam = pfam.long()\n",
    "        pfam= pfam.T.to(device) # batch_size must be the 2nd dimension so we use .T\n",
    "        # torch.transpose(pfam, 0, 1) it works like pfam.T\n",
    "        response = response.to(device)\n",
    "        \n",
    "        # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "        predictions = model(pfam).squeeze(1)\n",
    "                \n",
    "        loss = criterion(predictions, response)\n",
    "        \n",
    "        rounded_preds = torch.round(predictions)\n",
    "        correct = (rounded_preds == response).float()\n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "x torch.Size([1456, 5])\n",
      "x torch.Size([1456, 5, 200])\n",
      "output torch.Size([1456, 5, 256])\n",
      "hidden torch.Size([1, 5, 256])\n",
      "hidden_1D torch.Size([5, 256])\n",
      "last torch.Size([5, 1])\n",
      "| Epoch: 01 | Train Loss: 0.698 | Train Acc: 75.81% \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(vast_rnn, trainloader, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vast_rnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define another model with 2 lstm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_2_layers(nn.Module):\n",
    "    def __init__ (self, input_dim, embed_dim, hidden_dim, output_dim=1, n_layers =2):\n",
    "        super().__init__()\n",
    "        \n",
    "        embed_weights = np.loadtxt('embed_weights.txt') # load the pretrained weights\n",
    "        tensor_weight = torch.FloatTensor(embed_weights) # convert the pretrained weights into tensors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim).from_pretrained(tensor_weight)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"x\", x.shape) = torch.Size([3166, 5])\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # print(\"x\", x.shape) = torch.Size([3166, 5, 100])\n",
    "            \n",
    "        output, hidden = self.gru(x)\n",
    "        # print(\"output\", output.shape)  = torch.Size([3166, 5, 256])\n",
    "        # print(\"hidden\", hidden.shape) = torch.Size([2, 5, 256])\n",
    "        \n",
    "        #hidden = output[-1]\n",
    "        hidden = torch.cat((hidden[-1, :, :], hidden[-2, :, :]), dim = 1)\n",
    "        # print(\"hidden_1D\", hidden.shape) = torch.Size([5, 512])\n",
    "        \n",
    "        # print(self.fc(hidden).shape) = torch.Size([5, 1])\n",
    "        x = torch.sigmoid(self.fc(hidden))\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = vocab_dim\n",
    "embed_dim = 200\n",
    "hidden_dim = 256\n",
    "output_dim = 1 # binary classification has only 1 neuron in the last layer\n",
    "n_layers = 2 # 2 layers\n",
    "\n",
    "\n",
    "vast_gru_2 = RNN_2_layers(input_dim, embed_dim, hidden_dim, output_dim, n_layers=2)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vast_gru_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    \n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for pfam, response in iterator:\n",
    "        \n",
    "            pfam= pfam.T.to(device) # batch_size must be the 2nd dimension so we use .T\n",
    "            pfam = pfam.long()\n",
    "            # torch.transpose(pfam, 0, 1) it works like transpose\n",
    "            response = response.to(device)\n",
    "\n",
    "            # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "            predictions = model(pfam).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, response)\n",
    "\n",
    "            rounded_preds = torch.round(predictions)\n",
    "            correct = (rounded_preds == response).float()\n",
    "\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 2: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 3: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 4: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 5: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 6: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 7: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 8: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 9: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 10: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 11: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 12: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 13: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 14: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n",
      "Epoch 15: Train Loss: 0.693 | Train Acc: 76.13%\n",
      "\t Loss: 0.693 |  Val. Acc: 76.12%\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(vast_gru_2, trainloader, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(vast_gru_2, testloader, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(vast_gru_2.state_dict(), 'models/vast_gru_2_saved_weights.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vast_gru_2\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# del Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_bi(nn.Module):\n",
    "    def __init__ (self, input_dim, embed_dim, hidden_dim, output_dim=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional = bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\"x\", x.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        print(\"x\", x.shape)\n",
    "            \n",
    "        output, hidden = self.gru(x)\n",
    "        print(\"output\", output.shape) \n",
    "        print(\"hidden\", hidden.shape) \n",
    "        \n",
    "        hidden = torch.cat((hidden[-1,: ,:], hidden[-2, :, :]), dim = 1)\n",
    "        print(\"hidden_1D\", hidden.shape)\n",
    "        \n",
    "        print(self.fc(hidden.squeeze(0)).shape)\n",
    "        x = torch.sigmoid(self.fc(hidden))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = vocab_dim\n",
    "embed_dim = 100\n",
    "hidden_dim = 512\n",
    "output_dim = 1 # binary classification has only 1 neuron in the last layer\n",
    "\n",
    "\n",
    "vast_gru_bi = RNN_bi(input_dim, embed_dim, hidden_dim, output_dim)\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vast_gru_bi.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(vast_gru_bi, trainloader, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vast_gru_bi\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_bi_2(nn.Module):\n",
    "    def __init__ (self, input_dim, embed_dim, hidden_dim, output_dim=1, n_layers = 2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, n_layers, bidirectional = bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\"x\", x.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        print(\"x\", x.shape)\n",
    "            \n",
    "        output, hidden = self.gru(x)\n",
    "        print(\"output\", output.shape) \n",
    "        print(\"hidden\", hidden.shape) \n",
    "        \n",
    "        hidden = torch.cat((hidden[-1,: ,:], hidden[-2, :, :]), dim = 1)\n",
    "        print(\"hidden_1D\", hidden.shape)\n",
    "        \n",
    "        print(self.fc(hidden).shape)\n",
    "        x = torch.sigmoid(self.fc(hidden))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = vocab_dim\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1 # binary classification has only 1 neuron in the last layer\n",
    "\n",
    "\n",
    "vast_gru_bi_2 = RNN_bi_2(input_dim, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vast_gru_bi_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vast_gru_bi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(vast_gru_bi_2, trainloader, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "del vast_gru_bi_2\n",
    "# del Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "- Tabular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=training_set, batch_size=5, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_set, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(1456, 256) \n",
    "        self.layer_2 = nn.Linear(256, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(\"inputs\", inputs.shape) = torch.Size([5, 3166])\n",
    "        \n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        #print(\"layer_1\", x.shape) = torch.Size([5, 256])\n",
    "        \n",
    "#         x = self.batchnorm1(x)\n",
    "#         print(\"batchnorm1\", x.shape)\n",
    "        \n",
    "        x = self.relu(self.layer_2(x))\n",
    "        #print(\"layer_2\", x.shape) = torch.Size([5, 64])\n",
    "        \n",
    "#         x = self.batchnorm2(x)\n",
    "#         print(\"batchnorm2\", x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #print(\"dropout\", x.shape) = torch.Size([5, 64])\n",
    "               \n",
    "        x = torch.sigmoid(self.layer_out(x))\n",
    "        #print(\"layer_out\", x.shape) = torch.Size([5, 1])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=1456, out_features=256, bias=True)\n",
      "  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "- Input to the Linear layer must be `(batch_size, num_features)` so we shouldn't use `.T` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(model, trainloader, optimizer, criterion):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for pfam, response in trainloader: # shpuld be changed to \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pfam = pfam.long()\n",
    "        pfam= pfam.to(device) # batch_size must be the 1st dimension so we don't use .T\n",
    "        pfam = pfam.float()\n",
    "        # torch.transpose(pfam, 0, 1) it works like transpose\n",
    "        response = response.to(device)\n",
    "        \n",
    "        # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "        predictions = model(pfam).squeeze(1)\n",
    "                \n",
    "        loss = criterion(predictions, response)\n",
    "        \n",
    "        rounded_preds = torch.round(predictions)\n",
    "        correct = (rounded_preds == response).float()\n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(trainloader), epoch_acc / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.590 | Train Acc: 76.13% \n",
      "| Epoch: 02 | Train Loss: 0.465 | Train Acc: 76.13% \n",
      "| Epoch: 03 | Train Loss: 0.390 | Train Acc: 80.00% \n",
      "| Epoch: 04 | Train Loss: 0.318 | Train Acc: 85.81% \n",
      "| Epoch: 05 | Train Loss: 0.256 | Train Acc: 88.06% \n",
      "| Epoch: 06 | Train Loss: 0.207 | Train Acc: 91.29% \n",
      "| Epoch: 07 | Train Loss: 0.201 | Train Acc: 91.29% \n",
      "| Epoch: 08 | Train Loss: 0.174 | Train Acc: 91.94% \n",
      "| Epoch: 09 | Train Loss: 0.168 | Train Acc: 92.58% \n",
      "| Epoch: 10 | Train Loss: 0.161 | Train Acc: 92.58% \n",
      "| Epoch: 11 | Train Loss: 0.153 | Train Acc: 92.90% \n",
      "| Epoch: 12 | Train Loss: 0.137 | Train Acc: 93.23% \n",
      "| Epoch: 13 | Train Loss: 0.137 | Train Acc: 94.19% \n",
      "| Epoch: 14 | Train Loss: 0.140 | Train Acc: 93.55% \n",
      "| Epoch: 15 | Train Loss: 0.141 | Train Acc: 93.55% \n",
      "| Epoch: 16 | Train Loss: 0.129 | Train Acc: 93.55% \n",
      "| Epoch: 17 | Train Loss: 0.130 | Train Acc: 92.90% \n",
      "| Epoch: 18 | Train Loss: 0.118 | Train Acc: 94.84% \n",
      "| Epoch: 19 | Train Loss: 0.111 | Train Acc: 95.48% \n",
      "| Epoch: 20 | Train Loss: 0.104 | Train Acc: 95.16% \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train_linear(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import os\n",
    "torch.save(model.state_dict(), r'./models')\n",
    "torch.save(model.state_dict(), os.path.join(r'./models', 'linear_model.pkl'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/linear_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binaryClassification(\n",
       "  (layer_1): Linear(in_features=1456, out_features=256, bias=True)\n",
       "  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r\"./models/linear_model.pkl\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for pfam, response in iterator:\n",
    "        \n",
    "            pfam= pfam.to(device) # batch_size must be the 1st dimension so we don't use .T\n",
    "            pfam = pfam.float() # the model expects tensor of float type \n",
    "          \n",
    "            response = response.to(device)\n",
    "\n",
    "            # output of the model is: (batch_size, 1) and we get rid of 1 with sueeze(dim=1)\n",
    "            predictions = model(pfam).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, response)\n",
    "\n",
    "            rounded_preds = torch.round(predictions)\n",
    "            correct = (rounded_preds == response).float()\n",
    "\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.103 | Train Acc: 94.19%\n",
      "\t Loss: 1.728 |  Val. Acc: 74.63%\n",
      "----------------------------------------------------\n",
      "Epoch 2: Train Loss: 0.097 | Train Acc: 95.16%\n",
      "\t Loss: 1.835 |  Val. Acc: 67.91%\n",
      "----------------------------------------------------\n",
      "Epoch 3: Train Loss: 0.119 | Train Acc: 94.19%\n",
      "\t Loss: 1.683 |  Val. Acc: 71.64%\n",
      "----------------------------------------------------\n",
      "Epoch 4: Train Loss: 0.101 | Train Acc: 94.19%\n",
      "\t Loss: 1.785 |  Val. Acc: 70.90%\n",
      "----------------------------------------------------\n",
      "Epoch 5: Train Loss: 0.119 | Train Acc: 94.52%\n",
      "\t Loss: 1.675 |  Val. Acc: 71.64%\n",
      "----------------------------------------------------\n",
      "Epoch 6: Train Loss: 0.098 | Train Acc: 95.81%\n",
      "\t Loss: 1.834 |  Val. Acc: 72.39%\n",
      "----------------------------------------------------\n",
      "Epoch 7: Train Loss: 0.090 | Train Acc: 95.16%\n",
      "\t Loss: 1.946 |  Val. Acc: 73.13%\n",
      "----------------------------------------------------\n",
      "Epoch 8: Train Loss: 0.085 | Train Acc: 96.13%\n",
      "\t Loss: 1.940 |  Val. Acc: 70.15%\n",
      "----------------------------------------------------\n",
      "Epoch 9: Train Loss: 0.078 | Train Acc: 96.45%\n",
      "\t Loss: 2.113 |  Val. Acc: 69.40%\n",
      "----------------------------------------------------\n",
      "Epoch 10: Train Loss: 0.099 | Train Acc: 95.16%\n",
      "\t Loss: 2.039 |  Val. Acc: 73.88%\n",
      "----------------------------------------------------\n",
      "Epoch 11: Train Loss: 0.078 | Train Acc: 96.45%\n",
      "\t Loss: 2.105 |  Val. Acc: 74.63%\n",
      "----------------------------------------------------\n",
      "Epoch 12: Train Loss: 0.077 | Train Acc: 96.13%\n",
      "\t Loss: 2.196 |  Val. Acc: 70.90%\n",
      "----------------------------------------------------\n",
      "Epoch 13: Train Loss: 0.071 | Train Acc: 96.77%\n",
      "\t Loss: 2.237 |  Val. Acc: 70.15%\n",
      "----------------------------------------------------\n",
      "Epoch 14: Train Loss: 0.073 | Train Acc: 96.77%\n",
      "\t Loss: 2.220 |  Val. Acc: 71.64%\n",
      "----------------------------------------------------\n",
      "Epoch 15: Train Loss: 0.060 | Train Acc: 97.10%\n",
      "\t Loss: 2.396 |  Val. Acc: 70.15%\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train_linear(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate_linear(model, test_loader, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models\\linear_model_saved_weights.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.tensor(testing_set, dtype = torch.float32)\n",
    "testing_set = testing_set[:][0].type(torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_tensor = testing_set\n",
    "#testing_set[:][0]\n",
    "test_input_tensor.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = ig.attribute(test_input_tensor.to(device), target = 0 , n_steps=150)\n",
    "attr = attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -0.,  0., ...,  0.,  0., -0.],\n",
       "       [ 0., -0.,  0., ...,  0.,  0., -0.],\n",
       "       [ 0., -0.,  0., ...,  0.,  0., -0.],\n",
       "       ...,\n",
       "       [ 0., -0.,  0., ...,  0.,  0., -0.],\n",
       "       [ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0., -0.,  0., ...,  0.,  0., -0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 1456)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PF00218</th>\n",
       "      <th>PF00291</th>\n",
       "      <th>PF00290</th>\n",
       "      <th>PF02146</th>\n",
       "      <th>PF01649</th>\n",
       "      <th>PF02518</th>\n",
       "      <th>PF00512</th>\n",
       "      <th>PF00486</th>\n",
       "      <th>PF00072</th>\n",
       "      <th>PF05681</th>\n",
       "      <th>...</th>\n",
       "      <th>PF15902</th>\n",
       "      <th>PF09822</th>\n",
       "      <th>PF03739</th>\n",
       "      <th>PF13690</th>\n",
       "      <th>PF04509</th>\n",
       "      <th>PF10639</th>\n",
       "      <th>PF04439</th>\n",
       "      <th>PF12412</th>\n",
       "      <th>PF16347</th>\n",
       "      <th>PF02995</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 1456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PF00218  PF00291  PF00290  PF02146  PF01649  PF02518  PF00512  PF00486  \\\n",
       "0          0        0        0        0        0        0        0        0   \n",
       "1          0        0        0        0        0        0        0        0   \n",
       "2          0        0        0        0        0        0        0        0   \n",
       "3          0        0        0        0        0        0        0        0   \n",
       "4          0        0        0        0        0        0        0        0   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "129        0        0        0        0        0        0        0        0   \n",
       "130        0        0        0        0        0        0        0        0   \n",
       "131        0        0        0        0        0        0        0        0   \n",
       "132        0        0        0        0        0        0        0        0   \n",
       "133        0        0        0        0        0        0        0        0   \n",
       "\n",
       "     PF00072  PF05681  ...  PF15902  PF09822  PF03739  PF13690  PF04509  \\\n",
       "0          1        0  ...        0        0        0        0        0   \n",
       "1          0        0  ...        0        0        0        0        0   \n",
       "2          0        0  ...        0        0        0        0        0   \n",
       "3          0        0  ...        0        0        0        0        0   \n",
       "4          0        0  ...        0        0        0        0        0   \n",
       "..       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "129        0        0  ...        0        0        0        0        0   \n",
       "130        1        0  ...        0        0        0        0        0   \n",
       "131        0        0  ...        0        0        0        0        0   \n",
       "132        0        0  ...        0        0        0        0        0   \n",
       "133        0        0  ...        0        0        0        0        0   \n",
       "\n",
       "     PF10639  PF04439  PF12412  PF16347  PF02995  \n",
       "0          0        0        0        0        0  \n",
       "1          0        0        0        0        0  \n",
       "2          0        0        0        0        0  \n",
       "3          0        0        0        0        0  \n",
       "4          0        0        0        0        0  \n",
       "..       ...      ...      ...      ...      ...  \n",
       "129        0        0        0        0        0  \n",
       "130        0        0        0        0        0  \n",
       "131        0        0        0        0        0  \n",
       "132        0        0        0        0        0  \n",
       "133        0        0        0        0        0  \n",
       "\n",
       "[134 rows x 1456 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PF00218',\n",
       " 'PF00291',\n",
       " 'PF00290',\n",
       " 'PF02146',\n",
       " 'PF01649',\n",
       " 'PF02518',\n",
       " 'PF00512',\n",
       " 'PF00486',\n",
       " 'PF00072',\n",
       " 'PF05681',\n",
       " 'PF00254',\n",
       " 'PF05698',\n",
       " 'PF00574',\n",
       " 'PF06689',\n",
       " 'PF05496',\n",
       " 'PF07724',\n",
       " 'PF07728',\n",
       " 'PF00004',\n",
       " 'PF10431',\n",
       " 'PF05362',\n",
       " 'PF13541',\n",
       " 'PF01926',\n",
       " 'PF02421',\n",
       " 'PF00381',\n",
       " 'PF00160',\n",
       " 'PF08543',\n",
       " 'PF00294',\n",
       " 'PF03547',\n",
       " 'PF13165',\n",
       " 'PF13353',\n",
       " 'PF04055',\n",
       " 'PF07549',\n",
       " 'PF03176',\n",
       " 'PF02355',\n",
       " 'PF01368',\n",
       " 'PF02272',\n",
       " 'PF09084',\n",
       " 'PF00654',\n",
       " 'PF01928',\n",
       " 'PF00216',\n",
       " 'PF16326',\n",
       " 'PF00005',\n",
       " 'PF04616',\n",
       " 'PF13561',\n",
       " 'PF00106',\n",
       " 'PF01842',\n",
       " 'PF00754',\n",
       " 'PF08659',\n",
       " 'PF00501',\n",
       " 'PF14535',\n",
       " 'PF00109',\n",
       " 'PF02801',\n",
       " 'PF00550',\n",
       " 'PF00890',\n",
       " 'PF01266',\n",
       " 'PF13450',\n",
       " 'PF01593',\n",
       " 'PF13279',\n",
       " 'PF03061',\n",
       " 'PF03706',\n",
       " 'PF13620',\n",
       " 'PF13715',\n",
       " 'PF01066',\n",
       " 'PF02502',\n",
       " 'PF02779',\n",
       " 'PF00456',\n",
       " 'PF02782',\n",
       " 'PF00370',\n",
       " 'PF11762',\n",
       " 'PF00596',\n",
       " 'PF00293',\n",
       " 'PF13970',\n",
       " 'PF07977',\n",
       " 'PF08011',\n",
       " 'PF09820',\n",
       " 'PF03548',\n",
       " 'PF01522',\n",
       " 'PF04383',\n",
       " 'PF13723',\n",
       " 'PF08545',\n",
       " 'PF13489',\n",
       " 'PF00891',\n",
       " 'PF03279',\n",
       " 'PF01370',\n",
       " 'PF00221',\n",
       " 'PF09190',\n",
       " 'PF01406',\n",
       " 'PF09334',\n",
       " 'PF07523',\n",
       " 'PF07980',\n",
       " 'PF14322',\n",
       " 'PF00593',\n",
       " 'PF07715',\n",
       " 'PF14509',\n",
       " 'PF10566',\n",
       " 'PF14508',\n",
       " 'PF12833',\n",
       " 'PF00165',\n",
       " 'PF07495',\n",
       " 'PF07494',\n",
       " 'PF05977',\n",
       " 'PF07690',\n",
       " 'PF01032',\n",
       " 'PF01078',\n",
       " 'PF00975',\n",
       " 'PF13193',\n",
       " 'PF00668',\n",
       " 'PF03621',\n",
       " 'PF00756',\n",
       " 'PF00326',\n",
       " 'PF11806',\n",
       " 'PF14905',\n",
       " 'PF01648',\n",
       " 'PF02780',\n",
       " 'PF00676',\n",
       " 'PF02775',\n",
       " 'PF13292',\n",
       " 'PF04198',\n",
       " 'PF00532',\n",
       " 'PF13407',\n",
       " 'PF13377',\n",
       " 'PF02653',\n",
       " 'PF00126',\n",
       " 'PF03466',\n",
       " 'PF00924',\n",
       " 'PF02773',\n",
       " 'PF02772',\n",
       " 'PF00438',\n",
       " 'PF02719',\n",
       " 'PF16363',\n",
       " 'PF01073',\n",
       " 'PF07993',\n",
       " 'PF13460',\n",
       " 'PF00664',\n",
       " 'PF01408',\n",
       " 'PF13394',\n",
       " 'PF03711',\n",
       " 'PF01276',\n",
       " 'PF00155',\n",
       " 'PF03023',\n",
       " 'PF01554',\n",
       " 'PF14667',\n",
       " 'PF02899',\n",
       " 'PF06580',\n",
       " 'PF14310',\n",
       " 'PF07883',\n",
       " 'PF13472',\n",
       " 'PF01270',\n",
       " 'PF10503',\n",
       " 'PF16161',\n",
       " 'PF02829',\n",
       " 'PF08279',\n",
       " 'PF01729',\n",
       " 'PF02749',\n",
       " 'PF01494',\n",
       " 'PF02445',\n",
       " 'PF10111',\n",
       " 'PF00535',\n",
       " 'PF13641',\n",
       " 'PF03959',\n",
       " 'PF00561',\n",
       " 'PF12146',\n",
       " 'PF02129',\n",
       " 'PF02036',\n",
       " 'PF00440',\n",
       " 'PF00378',\n",
       " 'PF16113',\n",
       " 'PF14018',\n",
       " 'PF13240',\n",
       " 'PF10825',\n",
       " 'PF00128',\n",
       " 'PF06961',\n",
       " 'PF00152',\n",
       " 'PF01336',\n",
       " 'PF07745',\n",
       " 'PF00990',\n",
       " 'PF09148',\n",
       " 'PF09551',\n",
       " 'PF04073',\n",
       " 'PF01496',\n",
       " 'PF00137',\n",
       " 'PF01991',\n",
       " 'PF01992',\n",
       " 'PF01990',\n",
       " 'PF02874',\n",
       " 'PF16886',\n",
       " 'PF00006',\n",
       " 'PF01813',\n",
       " 'PF03729',\n",
       " 'PF14864',\n",
       " 'PF14863',\n",
       " 'PF00753',\n",
       " 'PF04397',\n",
       " 'PF14501',\n",
       " 'PF03417',\n",
       " 'PF02275',\n",
       " 'PF00579',\n",
       " 'PF01553',\n",
       " 'PF03358',\n",
       " 'PF02525',\n",
       " 'PF01734',\n",
       " 'PF13411',\n",
       " 'PF00376',\n",
       " 'PF09278',\n",
       " 'PF04542',\n",
       " 'PF08281',\n",
       " 'PF13490',\n",
       " 'PF01327',\n",
       " 'PF00583',\n",
       " 'PF13328',\n",
       " 'PF01966',\n",
       " 'PF04607',\n",
       " 'PF02824',\n",
       " 'PF13291',\n",
       " 'PF12706',\n",
       " 'PF02938',\n",
       " 'PF01569',\n",
       " 'PF13186',\n",
       " 'PF02311',\n",
       " 'PF00702',\n",
       " 'PF13419',\n",
       " 'PF01264',\n",
       " 'PF00534',\n",
       " 'PF13692',\n",
       " 'PF13524',\n",
       " 'PF04230',\n",
       " 'PF01943',\n",
       " 'PF03721',\n",
       " 'PF00984',\n",
       " 'PF13290',\n",
       " 'PF13287',\n",
       " 'PF08757',\n",
       " 'PF14907',\n",
       " 'PF13471',\n",
       " 'PF13537',\n",
       " 'PF00733',\n",
       " 'PF07475',\n",
       " 'PF05402',\n",
       " 'PF02397',\n",
       " 'PF04551',\n",
       " 'PF00731',\n",
       " 'PF04552',\n",
       " 'PF04963',\n",
       " 'PF00309',\n",
       " 'PF00575',\n",
       " 'PF00773',\n",
       " 'PF08206',\n",
       " 'PF01545',\n",
       " 'PF16916',\n",
       " 'PF04321',\n",
       " 'PF14378',\n",
       " 'PF00485',\n",
       " 'PF02690',\n",
       " 'PF01895',\n",
       " 'PF08541',\n",
       " 'PF00301',\n",
       " 'PF04932',\n",
       " 'PF01346',\n",
       " 'PF13404',\n",
       " 'PF13412',\n",
       " 'PF01037',\n",
       " 'PF07722',\n",
       " 'PF00117',\n",
       " 'PF01244',\n",
       " 'PF04389',\n",
       " 'PF02657',\n",
       " 'PF10417',\n",
       " 'PF08534',\n",
       " 'PF00578',\n",
       " 'PF00210',\n",
       " 'PF03544',\n",
       " 'PF12728',\n",
       " 'PF00156',\n",
       " 'PF01936',\n",
       " 'PF12872',\n",
       " 'PF00899',\n",
       " 'PF14460',\n",
       " 'PF14454',\n",
       " 'PF01131',\n",
       " 'PF01751',\n",
       " 'PF13351',\n",
       " 'PF13101',\n",
       " 'PF02954',\n",
       " 'PF14532',\n",
       " 'PF00158',\n",
       " 'PF13304',\n",
       " 'PF13175',\n",
       " 'PF13476',\n",
       " 'PF01047',\n",
       " 'PF12802',\n",
       " 'PF01325',\n",
       " 'PF03063',\n",
       " 'PF13487',\n",
       " 'PF07238',\n",
       " 'PF12670',\n",
       " 'PF13302',\n",
       " 'PF01841',\n",
       " 'PF01882',\n",
       " 'PF07726',\n",
       " 'PF14360',\n",
       " 'PF00860',\n",
       " 'PF13462',\n",
       " 'PF12804',\n",
       " 'PF00905',\n",
       " 'PF06100',\n",
       " 'PF10518',\n",
       " 'PF02872',\n",
       " 'PF12673',\n",
       " 'PF03591',\n",
       " 'PF05437',\n",
       " 'PF00941',\n",
       " 'PF03450',\n",
       " 'PF00483',\n",
       " 'PF12844',\n",
       " 'PF01381',\n",
       " 'PF13560',\n",
       " 'PF00383',\n",
       " 'PF01872',\n",
       " 'PF00926',\n",
       " 'PF08282',\n",
       " 'PF02699',\n",
       " 'PF07992',\n",
       " 'PF00070',\n",
       " 'PF14691',\n",
       " 'PF10418',\n",
       " 'PF09285',\n",
       " 'PF01132',\n",
       " 'PF08207',\n",
       " 'PF00557',\n",
       " 'PF01321',\n",
       " 'PF01220',\n",
       " 'PF13242',\n",
       " 'PF08264',\n",
       " 'PF00133',\n",
       " 'PF13603',\n",
       " 'PF13704',\n",
       " 'PF09835',\n",
       " 'PF05175',\n",
       " 'PF13847',\n",
       " 'PF13649',\n",
       " 'PF08241',\n",
       " 'PF08447',\n",
       " 'PF01590',\n",
       " 'PF02627',\n",
       " 'PF12682',\n",
       " 'PF01738',\n",
       " 'PF05448',\n",
       " 'PF07859',\n",
       " 'PF00135',\n",
       " 'PF13229',\n",
       " 'PF00682',\n",
       " 'PF00089',\n",
       " 'PF14572',\n",
       " 'PF13793',\n",
       " 'PF03372',\n",
       " 'PF13443',\n",
       " 'PF03006',\n",
       " 'PF02622',\n",
       " 'PF04519',\n",
       " 'PF00977',\n",
       " 'PF02594',\n",
       " 'PF01025',\n",
       " 'PF00226',\n",
       " 'PF01556',\n",
       " 'PF00684',\n",
       " 'PF04452',\n",
       " 'PF02152',\n",
       " 'PF00494',\n",
       " 'PF00717',\n",
       " 'PF02913',\n",
       " 'PF01565',\n",
       " 'PF11984',\n",
       " 'PF09721',\n",
       " 'PF01386',\n",
       " 'PF14693',\n",
       " 'PF00475',\n",
       " 'PF13241',\n",
       " 'PF01379',\n",
       " 'PF03900',\n",
       " 'PF00590',\n",
       " 'PF02602',\n",
       " 'PF00490',\n",
       " 'PF00202',\n",
       " 'PF02082',\n",
       " 'PF03444',\n",
       " 'PF01053',\n",
       " 'PF01041',\n",
       " 'PF03460',\n",
       " 'PF01077',\n",
       " 'PF12837',\n",
       " 'PF00037',\n",
       " 'PF12838',\n",
       " 'PF13187',\n",
       " 'PF13237',\n",
       " 'PF01206',\n",
       " 'PF00085',\n",
       " 'PF13098',\n",
       " 'PF07449',\n",
       " 'PF02597',\n",
       " 'PF01398',\n",
       " 'PF14464',\n",
       " 'PF13738',\n",
       " 'PF02910',\n",
       " 'PF12797',\n",
       " 'PF14697',\n",
       " 'PF13370',\n",
       " 'PF12800',\n",
       " 'PF01507',\n",
       " 'PF00009',\n",
       " 'PF05154',\n",
       " 'PF02870',\n",
       " 'PF01035',\n",
       " 'PF13581',\n",
       " 'PF01740',\n",
       " 'PF13466',\n",
       " 'PF00149',\n",
       " 'PF12850',\n",
       " 'PF12320',\n",
       " 'PF13514',\n",
       " 'PF13555',\n",
       " 'PF13558',\n",
       " 'PF02535',\n",
       " 'PF00122',\n",
       " 'PF13630',\n",
       " 'PF07853',\n",
       " 'PF12840',\n",
       " 'PF01022',\n",
       " 'PF03952',\n",
       " 'PF00113',\n",
       " 'PF13378',\n",
       " 'PF07476',\n",
       " 'PF13508',\n",
       " 'PF01757',\n",
       " 'PF04464',\n",
       " 'PF04260',\n",
       " 'PF13420',\n",
       " 'PF13673',\n",
       " 'PF08445',\n",
       " 'PF14492',\n",
       " 'PF03764',\n",
       " 'PF00679',\n",
       " 'PF05991',\n",
       " 'PF00480',\n",
       " 'PF00589',\n",
       " 'PF04854',\n",
       " 'PF13495',\n",
       " 'PF01944',\n",
       " 'PF03807',\n",
       " 'PF03446',\n",
       " 'PF01702',\n",
       " 'PF02687',\n",
       " 'PF00465',\n",
       " 'PF13685',\n",
       " 'PF01663',\n",
       " 'PF13840',\n",
       " 'PF05173',\n",
       " 'PF01113',\n",
       " 'PF00701',\n",
       " 'PF02774',\n",
       " 'PF01118',\n",
       " 'PF02547',\n",
       " 'PF01012',\n",
       " 'PF00766',\n",
       " 'PF00881',\n",
       " 'PF02771',\n",
       " 'PF02770',\n",
       " 'PF00441',\n",
       " 'PF08028',\n",
       " 'PF16197',\n",
       " 'PF01136',\n",
       " 'PF02543',\n",
       " 'PF16861',\n",
       " 'PF02737',\n",
       " 'PF00725',\n",
       " 'PF00698',\n",
       " 'PF00300',\n",
       " 'PF02310',\n",
       " 'PF01261',\n",
       " 'PF00356',\n",
       " 'PF00248',\n",
       " 'PF08240',\n",
       " 'PF00107',\n",
       " 'PF02837',\n",
       " 'PF02836',\n",
       " 'PF14681',\n",
       " 'PF16874',\n",
       " 'PF02065',\n",
       " 'PF16875',\n",
       " 'PF04041',\n",
       " 'PF16011',\n",
       " 'PF05891',\n",
       " 'PF05889',\n",
       " 'PF00266',\n",
       " 'PF02347',\n",
       " 'PF02646',\n",
       " 'PF00528',\n",
       " 'PF00497',\n",
       " 'PF00728',\n",
       " 'PF02838',\n",
       " 'PF00571',\n",
       " 'PF13185',\n",
       " 'PF07335',\n",
       " 'PF01061',\n",
       " 'PF13365',\n",
       " 'PF12773',\n",
       " 'PF04545',\n",
       " 'PF13463',\n",
       " 'PF00392',\n",
       " 'PF11799',\n",
       " 'PF00817',\n",
       " 'PF04326',\n",
       " 'PF13749',\n",
       " 'PF07486',\n",
       " 'PF06969',\n",
       " 'PF03144',\n",
       " 'PF16658',\n",
       " 'PF01212',\n",
       " 'PF07670',\n",
       " 'PF07664',\n",
       " 'PF04023',\n",
       " 'PF14478',\n",
       " 'PF02361',\n",
       " 'PF03773',\n",
       " 'PF13192',\n",
       " 'PF08242',\n",
       " 'PF00398',\n",
       " 'PF01795',\n",
       " 'PF00672',\n",
       " 'PF02321',\n",
       " 'PF11604',\n",
       " 'PF00529',\n",
       " 'PF16576',\n",
       " 'PF16572',\n",
       " 'PF13437',\n",
       " 'PF00873',\n",
       " 'PF00324',\n",
       " 'PF13520',\n",
       " 'PF04237',\n",
       " 'PF06643',\n",
       " 'PF04107',\n",
       " 'PF01848',\n",
       " 'PF02706',\n",
       " 'PF01497',\n",
       " 'PF00425',\n",
       " 'PF00857',\n",
       " 'PF02554',\n",
       " 'PF13722',\n",
       " 'PF04328',\n",
       " 'PF02195',\n",
       " 'PF11922',\n",
       " 'PF00582',\n",
       " 'PF01272',\n",
       " 'PF14760',\n",
       " 'PF00445',\n",
       " 'PF00939',\n",
       " 'PF03600',\n",
       " 'PF01874',\n",
       " 'PF03802',\n",
       " 'PF16193',\n",
       " 'PF12002',\n",
       " 'PF02403',\n",
       " 'PF00587',\n",
       " 'PF04879',\n",
       " 'PF00384',\n",
       " 'PF01568',\n",
       " 'PF13247',\n",
       " 'PF12798',\n",
       " 'PF04976',\n",
       " 'PF06779',\n",
       " 'PF00083',\n",
       " 'PF01228',\n",
       " 'PF02901',\n",
       " 'PF01226',\n",
       " 'PF02624',\n",
       " 'PF04239',\n",
       " 'PF00275',\n",
       " 'PF01435',\n",
       " 'PF13189',\n",
       " 'PF02224',\n",
       " 'PF03772',\n",
       " 'PF13439',\n",
       " 'PF02655',\n",
       " 'PF05116',\n",
       " 'PF13936',\n",
       " 'PF00665',\n",
       " 'PF04647',\n",
       " 'PF00588',\n",
       " 'PF01547',\n",
       " 'PF13416',\n",
       " 'PF04539',\n",
       " 'PF12710',\n",
       " 'PF01195',\n",
       " 'PF02559',\n",
       " 'PF04851',\n",
       " 'PF00270',\n",
       " 'PF00271',\n",
       " 'PF03461',\n",
       " 'PF13145',\n",
       " 'PF13616',\n",
       " 'PF00639',\n",
       " 'PF03215',\n",
       " 'PF01520',\n",
       " 'PF00154',\n",
       " 'PF06745',\n",
       " 'PF13481',\n",
       " 'PF12704',\n",
       " 'PF01502',\n",
       " 'PF07685',\n",
       " 'PF01425',\n",
       " 'PF00144',\n",
       " 'PF12697',\n",
       " 'PF08020',\n",
       " 'PF13356',\n",
       " 'PF01709',\n",
       " 'PF01048',\n",
       " 'PF10423',\n",
       " 'PF13347',\n",
       " 'PF01075',\n",
       " 'PF02369',\n",
       " 'PF09134',\n",
       " 'PF11924',\n",
       " 'PF10017',\n",
       " 'PF01209',\n",
       " 'PF00108',\n",
       " 'PF14659',\n",
       " 'PF06167',\n",
       " 'PF05930',\n",
       " 'PF07733',\n",
       " 'PF02811',\n",
       " 'PF05147',\n",
       " 'PF13575',\n",
       " 'PF01643',\n",
       " 'PF07729',\n",
       " 'PF01476',\n",
       " 'PF05401',\n",
       " 'PF02742',\n",
       " 'PF02661',\n",
       " 'PF00132',\n",
       " 'PF09989',\n",
       " 'PF01869',\n",
       " 'PF03435',\n",
       " 'PF13534',\n",
       " 'PF13602',\n",
       " 'PF01551',\n",
       " 'PF02163',\n",
       " 'PF02075',\n",
       " 'PF01330',\n",
       " 'PF14520',\n",
       " 'PF07499',\n",
       " 'PF01134',\n",
       " 'PF12831',\n",
       " 'PF03401',\n",
       " 'PF01970',\n",
       " 'PF12911',\n",
       " 'PF00496',\n",
       " 'PF08352',\n",
       " 'PF03483',\n",
       " 'PF04069',\n",
       " 'PF02613',\n",
       " 'PF01380',\n",
       " 'PF01418',\n",
       " 'PF04074',\n",
       " 'PF00474',\n",
       " 'PF04131',\n",
       " 'PF02581',\n",
       " 'PF02743',\n",
       " 'PF01546',\n",
       " 'PF05343',\n",
       " 'PF00912',\n",
       " 'PF01202',\n",
       " 'PF01613',\n",
       " 'PF02833',\n",
       " 'PF07085',\n",
       " 'PF02446',\n",
       " 'PF00343',\n",
       " 'PF00082',\n",
       " 'PF12784',\n",
       " 'PF01909',\n",
       " 'PF08780',\n",
       " 'PF00690',\n",
       " 'PF13246',\n",
       " 'PF00689',\n",
       " 'PF04474',\n",
       " 'PF04454',\n",
       " 'PF09950',\n",
       " 'PF13426',\n",
       " 'PF01964',\n",
       " 'PF02080',\n",
       " 'PF01434',\n",
       " 'PF06480',\n",
       " 'PF00849',\n",
       " 'PF04060',\n",
       " 'PF02508',\n",
       " 'PF04205',\n",
       " 'PF03116',\n",
       " 'PF11681',\n",
       " 'PF11863',\n",
       " 'PF13262',\n",
       " 'PF09979',\n",
       " 'PF04233',\n",
       " 'PF14206',\n",
       " 'PF00455',\n",
       " 'PF08220',\n",
       " 'PF01174',\n",
       " 'PF16927',\n",
       " 'PF00403',\n",
       " 'PF12679',\n",
       " 'PF12730',\n",
       " 'PF03960',\n",
       " 'PF07179',\n",
       " 'PF04101',\n",
       " 'PF13385',\n",
       " 'PF08522',\n",
       " 'PF16141',\n",
       " 'PF12741',\n",
       " 'PF12771',\n",
       " 'PF07660',\n",
       " 'PF16344',\n",
       " 'PF04773',\n",
       " 'PF00196',\n",
       " 'PF07638',\n",
       " 'PF08973',\n",
       " 'PF13248',\n",
       " 'PF04892',\n",
       " 'PF13459',\n",
       " 'PF07610',\n",
       " 'PF12849',\n",
       " 'PF00989',\n",
       " 'PF01595',\n",
       " 'PF03471',\n",
       " 'PF00437',\n",
       " 'PF02800',\n",
       " 'PF00044',\n",
       " 'PF06177',\n",
       " 'PF01558',\n",
       " 'PF01855',\n",
       " 'PF02776',\n",
       " 'PF01063',\n",
       " 'PF04026',\n",
       " 'PF01225',\n",
       " 'PF08245',\n",
       " 'PF02875',\n",
       " 'PF07261',\n",
       " 'PF15418',\n",
       " 'PF13614',\n",
       " 'PF10609',\n",
       " 'PF06564',\n",
       " 'PF09140',\n",
       " 'PF01656',\n",
       " 'PF00142',\n",
       " 'PF02374',\n",
       " 'PF01464',\n",
       " 'PF01411',\n",
       " 'PF07973',\n",
       " 'PF13538',\n",
       " 'PF00174',\n",
       " 'PF01794',\n",
       " 'PF09223',\n",
       " 'PF02283',\n",
       " 'PF12848',\n",
       " 'PF02810',\n",
       " 'PF12974',\n",
       " 'PF02110',\n",
       " 'PF08211',\n",
       " 'PF01832',\n",
       " 'PF10988',\n",
       " 'PF10412',\n",
       " 'PF12696',\n",
       " 'PF03050',\n",
       " 'PF03412',\n",
       " 'PF01624',\n",
       " 'PF00145',\n",
       " 'PF03703',\n",
       " 'PF13155',\n",
       " 'PF07751',\n",
       " 'PF05717',\n",
       " 'PF08443',\n",
       " 'PF02955',\n",
       " 'PF10458',\n",
       " 'PF08643',\n",
       " 'PF04986',\n",
       " 'PF00015',\n",
       " 'PF09587',\n",
       " 'PF03816',\n",
       " 'PF00933',\n",
       " 'PF01915',\n",
       " 'PF13375',\n",
       " 'PF05896',\n",
       " 'PF01512',\n",
       " 'PF10531',\n",
       " 'PF13204',\n",
       " 'PF12904',\n",
       " 'PF01263',\n",
       " 'PF00408',\n",
       " 'PF02880',\n",
       " 'PF02879',\n",
       " 'PF02878',\n",
       " 'PF16109',\n",
       " 'PF13567',\n",
       " 'PF00834',\n",
       " 'PF02911',\n",
       " 'PF00551',\n",
       " 'PF01300',\n",
       " 'PF09992',\n",
       " 'PF12668',\n",
       " 'PF13151',\n",
       " 'PF01925',\n",
       " 'PF03486',\n",
       " 'PF03022',\n",
       " 'PF03606',\n",
       " 'PF06808',\n",
       " 'PF01810',\n",
       " 'PF01946',\n",
       " 'PF01490',\n",
       " 'PF02458',\n",
       " 'PF01668',\n",
       " 'PF02588',\n",
       " 'PF10035',\n",
       " 'PF05860',\n",
       " 'PF05594',\n",
       " 'PF08479',\n",
       " 'PF17287',\n",
       " 'PF03865',\n",
       " 'PF02826',\n",
       " 'PF01661',\n",
       " 'PF13091',\n",
       " 'PF08032',\n",
       " 'PF13432',\n",
       " 'PF04733',\n",
       " 'PF14559',\n",
       " 'PF01042',\n",
       " 'PF13638',\n",
       " 'PF02562',\n",
       " 'PF13604',\n",
       " 'PF09848',\n",
       " 'PF13727',\n",
       " 'PF07228',\n",
       " 'PF00400',\n",
       " 'PF01471',\n",
       " 'PF13744',\n",
       " 'PF05973',\n",
       " 'PF02436',\n",
       " 'PF00364',\n",
       " 'PF10704',\n",
       " 'PF13477',\n",
       " 'PF13579',\n",
       " 'PF02583',\n",
       " 'PF00830',\n",
       " 'PF17293',\n",
       " 'PF13102',\n",
       " 'PF01637',\n",
       " 'PF13173',\n",
       " 'PF01609',\n",
       " 'PF05598',\n",
       " 'PF11756',\n",
       " 'PF13506',\n",
       " 'PF04240',\n",
       " 'PF06827',\n",
       " 'PF06831',\n",
       " 'PF02769',\n",
       " 'PF00920',\n",
       " 'PF01026',\n",
       " 'PF04909',\n",
       " 'PF13728',\n",
       " 'PF00657',\n",
       " 'PF13930',\n",
       " 'PF03796',\n",
       " 'PF12762',\n",
       " 'PF00401',\n",
       " 'PF02823',\n",
       " 'PF02894',\n",
       " 'PF02746',\n",
       " 'PF04204',\n",
       " 'PF04072',\n",
       " 'PF14765',\n",
       " 'PF00140',\n",
       " 'PF03979',\n",
       " 'PF13362',\n",
       " 'PF13662',\n",
       " 'PF08275',\n",
       " 'PF01807',\n",
       " 'PF00488',\n",
       " 'PF00436',\n",
       " 'PF00694',\n",
       " 'PF00330',\n",
       " 'PF00892',\n",
       " 'PF08238',\n",
       " 'PF00999',\n",
       " 'PF04226',\n",
       " 'PF00205',\n",
       " 'PF10369',\n",
       " 'PF08502',\n",
       " 'PF00180',\n",
       " 'PF00150',\n",
       " 'PF13522',\n",
       " 'PF06472',\n",
       " 'PF03432',\n",
       " 'PF00239',\n",
       " 'PF07508',\n",
       " 'PF13408',\n",
       " 'PF14287',\n",
       " 'PF02638',\n",
       " 'PF00563',\n",
       " 'PF09823',\n",
       " 'PF00078',\n",
       " 'PF08291',\n",
       " 'PF12224',\n",
       " 'PF02580',\n",
       " 'PF07862',\n",
       " 'PF07963',\n",
       " 'PF03977',\n",
       " 'PF00814',\n",
       " 'PF02367',\n",
       " 'PF02348',\n",
       " 'PF03323',\n",
       " 'PF03845',\n",
       " 'PF00464',\n",
       " 'PF13533',\n",
       " 'PF13336',\n",
       " 'PF02550',\n",
       " 'PF07478',\n",
       " 'PF02786',\n",
       " 'PF03129',\n",
       " 'PF00707',\n",
       " 'PF00453',\n",
       " 'PF05491',\n",
       " 'PF11188',\n",
       " 'PF17147',\n",
       " 'PF01008',\n",
       " 'PF12698',\n",
       " 'PF00809',\n",
       " 'PF09586',\n",
       " 'PF12715',\n",
       " 'PF13393',\n",
       " 'PF02492',\n",
       " 'PF00708',\n",
       " 'PF07503',\n",
       " 'PF01455',\n",
       " 'PF01924',\n",
       " 'PF00586',\n",
       " 'PF00012',\n",
       " 'PF06723',\n",
       " 'PF00166',\n",
       " 'PF00768',\n",
       " 'PF13354',\n",
       " 'PF14057',\n",
       " 'PF00691',\n",
       " 'PF13458',\n",
       " 'PF12385',\n",
       " 'PF00916',\n",
       " 'PF02915',\n",
       " 'PF00877',\n",
       " 'PF12650',\n",
       " 'PF02353',\n",
       " 'PF07589',\n",
       " 'PF03449',\n",
       " 'PF01259',\n",
       " 'PF00636',\n",
       " 'PF08239',\n",
       " 'PF06347',\n",
       " 'PF13180',\n",
       " 'PF00595',\n",
       " 'PF11007',\n",
       " 'PF12652',\n",
       " 'PF05067',\n",
       " 'PF07944',\n",
       " 'PF10000',\n",
       " 'PF12637',\n",
       " 'PF03070',\n",
       " 'PF01256',\n",
       " 'PF00389',\n",
       " 'PF01958',\n",
       " 'PF03447',\n",
       " 'PF00478',\n",
       " 'PF01070',\n",
       " 'PF02384',\n",
       " 'PF05954',\n",
       " 'PF17541',\n",
       " 'PF02368',\n",
       " 'PF00793',\n",
       " 'PF01761',\n",
       " 'PF14602',\n",
       " 'PF13648',\n",
       " 'PF05593',\n",
       " 'PF01479',\n",
       " 'PF00724',\n",
       " 'PF02861',\n",
       " 'PF12464',\n",
       " 'PF13580',\n",
       " 'PF14446',\n",
       " 'PF04203',\n",
       " 'PF01475',\n",
       " 'PF02922',\n",
       " 'PF13245',\n",
       " 'PF08840',\n",
       " 'PF13382',\n",
       " 'PF01979',\n",
       " 'PF01297',\n",
       " 'PF00950',\n",
       " 'PF02498',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(test_dataset.columns[2:])\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Feature Importances\n",
      "PF00218 :  -0.000\n",
      "PF00291 :  -0.001\n",
      "PF00290 :  0.000\n",
      "PF02146 :  0.000\n",
      "PF01649 :  0.000\n",
      "PF02518 :  -0.017\n",
      "PF00512 :  0.001\n",
      "PF00486 :  0.001\n",
      "PF00072 :  0.023\n",
      "PF05681 :  -0.000\n",
      "PF00254 :  0.001\n",
      "PF05698 :  0.002\n",
      "PF00574 :  0.000\n",
      "PF06689 :  0.000\n",
      "PF05496 :  0.004\n",
      "PF07724 :  0.000\n",
      "PF07728 :  -0.000\n",
      "PF00004 :  0.003\n",
      "PF10431 :  0.000\n",
      "PF05362 :  -0.000\n",
      "PF13541 :  -0.000\n",
      "PF01926 :  0.000\n",
      "PF02421 :  -0.000\n",
      "PF00381 :  -0.000\n",
      "PF00160 :  -0.000\n",
      "PF08543 :  -0.002\n",
      "PF00294 :  -0.002\n",
      "PF03547 :  -0.001\n",
      "PF13165 :  -0.025\n",
      "PF13353 :  -0.010\n",
      "PF04055 :  0.000\n",
      "PF07549 :  -0.005\n",
      "PF03176 :  0.055\n",
      "PF02355 :  -0.006\n",
      "PF01368 :  -0.013\n",
      "PF02272 :  -0.006\n",
      "PF09084 :  -0.001\n",
      "PF00654 :  -0.009\n",
      "PF01928 :  -0.001\n",
      "PF00216 :  -0.001\n",
      "PF16326 :  -0.001\n",
      "PF00005 :  0.017\n",
      "PF04616 :  -0.006\n",
      "PF13561 :  -0.030\n",
      "PF00106 :  -0.027\n",
      "PF01842 :  -0.028\n",
      "PF00754 :  -0.006\n",
      "PF08659 :  -0.013\n",
      "PF00501 :  0.006\n",
      "PF14535 :  -0.000\n",
      "PF00109 :  0.033\n",
      "PF02801 :  0.063\n",
      "PF00550 :  0.011\n",
      "PF00890 :  -0.034\n",
      "PF01266 :  -0.006\n",
      "PF13450 :  0.007\n",
      "PF01593 :  -0.000\n",
      "PF13279 :  0.022\n",
      "PF03061 :  0.019\n",
      "PF03706 :  -0.002\n",
      "PF13620 :  0.023\n",
      "PF13715 :  0.019\n",
      "PF01066 :  -0.000\n",
      "PF02502 :  -0.003\n",
      "PF02779 :  -0.000\n",
      "PF00456 :  -0.000\n",
      "PF02782 :  -0.009\n",
      "PF00370 :  -0.000\n",
      "PF11762 :  0.000\n",
      "PF00596 :  0.000\n",
      "PF00293 :  -0.015\n",
      "PF13970 :  -0.006\n",
      "PF07977 :  -0.004\n",
      "PF08011 :  0.000\n",
      "PF09820 :  0.000\n",
      "PF03548 :  0.006\n",
      "PF01522 :  -0.026\n",
      "PF04383 :  -0.007\n",
      "PF13723 :  -0.028\n",
      "PF08545 :  -0.012\n",
      "PF13489 :  -0.002\n",
      "PF00891 :  0.024\n",
      "PF03279 :  -0.016\n",
      "PF01370 :  -0.009\n",
      "PF00221 :  -0.015\n",
      "PF09190 :  0.011\n",
      "PF01406 :  0.012\n",
      "PF09334 :  0.002\n",
      "PF07523 :  -0.002\n",
      "PF07980 :  0.002\n",
      "PF14322 :  0.014\n",
      "PF00593 :  0.023\n",
      "PF07715 :  0.032\n",
      "PF14509 :  -0.008\n",
      "PF10566 :  0.004\n",
      "PF14508 :  -0.007\n",
      "PF12833 :  -0.006\n",
      "PF00165 :  -0.005\n",
      "PF07495 :  0.005\n",
      "PF07494 :  -0.000\n",
      "PF05977 :  0.000\n",
      "PF07690 :  -0.011\n",
      "PF01032 :  0.000\n",
      "PF01078 :  -0.000\n",
      "PF00975 :  -0.002\n",
      "PF13193 :  0.015\n",
      "PF00668 :  -0.003\n",
      "PF03621 :  0.000\n",
      "PF00756 :  -0.002\n",
      "PF00326 :  0.004\n",
      "PF11806 :  0.000\n",
      "PF14905 :  0.001\n",
      "PF01648 :  -0.014\n",
      "PF02780 :  0.000\n",
      "PF00676 :  0.000\n",
      "PF02775 :  0.019\n",
      "PF13292 :  0.000\n",
      "PF04198 :  0.000\n",
      "PF00532 :  -0.001\n",
      "PF13407 :  -0.000\n",
      "PF13377 :  -0.001\n",
      "PF02653 :  -0.001\n",
      "PF00126 :  -0.000\n",
      "PF03466 :  0.003\n",
      "PF00924 :  0.000\n",
      "PF02773 :  -0.000\n",
      "PF02772 :  -0.000\n",
      "PF00438 :  0.000\n",
      "PF02719 :  -0.001\n",
      "PF16363 :  0.000\n",
      "PF01073 :  0.000\n",
      "PF07993 :  0.000\n",
      "PF13460 :  0.000\n",
      "PF00664 :  0.000\n",
      "PF01408 :  -0.006\n",
      "PF13394 :  0.018\n",
      "PF03711 :  0.000\n",
      "PF01276 :  -0.009\n",
      "PF00155 :  -0.000\n",
      "PF03023 :  0.001\n",
      "PF01554 :  0.002\n",
      "PF14667 :  0.001\n",
      "PF02899 :  0.000\n",
      "PF06580 :  -0.005\n",
      "PF14310 :  -0.000\n",
      "PF07883 :  0.001\n",
      "PF13472 :  0.000\n",
      "PF01270 :  -0.001\n",
      "PF10503 :  0.000\n",
      "PF16161 :  -0.000\n",
      "PF02829 :  0.000\n",
      "PF08279 :  -0.001\n",
      "PF01729 :  0.000\n",
      "PF02749 :  0.000\n",
      "PF01494 :  0.014\n",
      "PF02445 :  -0.001\n",
      "PF10111 :  -0.000\n",
      "PF00535 :  0.046\n",
      "PF13641 :  -0.004\n",
      "PF03959 :  -0.000\n",
      "PF00561 :  0.000\n",
      "PF12146 :  -0.000\n",
      "PF02129 :  -0.000\n",
      "PF02036 :  -0.000\n",
      "PF00440 :  -0.004\n",
      "PF00378 :  0.001\n",
      "PF16113 :  0.001\n",
      "PF14018 :  0.000\n",
      "PF13240 :  0.000\n",
      "PF10825 :  0.000\n",
      "PF00128 :  0.000\n",
      "PF06961 :  -0.004\n",
      "PF00152 :  -0.003\n",
      "PF01336 :  -0.003\n",
      "PF07745 :  0.000\n",
      "PF00990 :  -0.001\n",
      "PF09148 :  0.001\n",
      "PF09551 :  0.000\n",
      "PF04073 :  0.000\n",
      "PF01496 :  0.000\n",
      "PF00137 :  0.000\n",
      "PF01991 :  0.000\n",
      "PF01992 :  0.000\n",
      "PF01990 :  0.000\n",
      "PF02874 :  0.000\n",
      "PF16886 :  0.000\n",
      "PF00006 :  0.000\n",
      "PF01813 :  0.000\n",
      "PF03729 :  -0.000\n",
      "PF14864 :  -0.000\n",
      "PF14863 :  -0.000\n",
      "PF00753 :  0.004\n",
      "PF04397 :  -0.042\n",
      "PF14501 :  0.005\n",
      "PF03417 :  -0.003\n",
      "PF02275 :  -0.000\n",
      "PF00579 :  -0.000\n",
      "PF01553 :  0.021\n",
      "PF03358 :  -0.005\n",
      "PF02525 :  -0.007\n",
      "PF01734 :  -0.000\n",
      "PF13411 :  -0.007\n",
      "PF00376 :  -0.009\n",
      "PF09278 :  0.000\n",
      "PF04542 :  0.006\n",
      "PF08281 :  0.002\n",
      "PF13490 :  -0.000\n",
      "PF01327 :  -0.000\n",
      "PF00583 :  0.001\n",
      "PF13328 :  0.010\n",
      "PF01966 :  0.006\n",
      "PF04607 :  0.013\n",
      "PF02824 :  0.008\n",
      "PF13291 :  0.007\n",
      "PF12706 :  -0.000\n",
      "PF02938 :  0.000\n",
      "PF01569 :  0.001\n",
      "PF13186 :  -0.030\n",
      "PF02311 :  0.028\n",
      "PF00702 :  -0.007\n",
      "PF13419 :  -0.005\n",
      "PF01264 :  0.000\n",
      "PF00534 :  -0.000\n",
      "PF13692 :  -0.000\n",
      "PF13524 :  -0.001\n",
      "PF04230 :  0.000\n",
      "PF01943 :  -0.000\n",
      "PF03721 :  0.000\n",
      "PF00984 :  0.000\n",
      "PF13290 :  0.003\n",
      "PF13287 :  0.006\n",
      "PF08757 :  0.000\n",
      "PF14907 :  0.000\n",
      "PF13471 :  0.000\n",
      "PF13537 :  0.000\n",
      "PF00733 :  0.000\n",
      "PF07475 :  0.000\n",
      "PF05402 :  0.000\n",
      "PF02397 :  -0.000\n",
      "PF04551 :  0.000\n",
      "PF00731 :  0.000\n",
      "PF04552 :  0.000\n",
      "PF04963 :  0.000\n",
      "PF00309 :  0.000\n",
      "PF00575 :  0.000\n",
      "PF00773 :  0.000\n",
      "PF08206 :  0.000\n",
      "PF01545 :  0.000\n",
      "PF16916 :  0.000\n",
      "PF04321 :  0.000\n",
      "PF14378 :  0.001\n",
      "PF00485 :  0.000\n",
      "PF02690 :  0.000\n",
      "PF01895 :  0.000\n",
      "PF08541 :  -0.002\n",
      "PF00301 :  0.000\n",
      "PF04932 :  0.000\n",
      "PF01346 :  0.000\n",
      "PF13404 :  -0.000\n",
      "PF13412 :  -0.000\n",
      "PF01037 :  -0.000\n",
      "PF07722 :  -0.000\n",
      "PF00117 :  -0.000\n",
      "PF01244 :  0.000\n",
      "PF04389 :  0.000\n",
      "PF02657 :  0.000\n",
      "PF10417 :  -0.000\n",
      "PF08534 :  -0.001\n",
      "PF00578 :  -0.001\n",
      "PF00210 :  -0.003\n",
      "PF03544 :  0.000\n",
      "PF12728 :  0.000\n",
      "PF00156 :  0.008\n",
      "PF01936 :  0.000\n",
      "PF12872 :  0.000\n",
      "PF00899 :  0.001\n",
      "PF14460 :  0.000\n",
      "PF14454 :  0.000\n",
      "PF01131 :  0.000\n",
      "PF01751 :  -0.001\n",
      "PF13351 :  -0.000\n",
      "PF13101 :  0.000\n",
      "PF02954 :  -0.001\n",
      "PF14532 :  -0.001\n",
      "PF00158 :  -0.001\n",
      "PF13304 :  -0.001\n",
      "PF13175 :  0.000\n",
      "PF13476 :  -0.000\n",
      "PF01047 :  -0.006\n",
      "PF12802 :  0.003\n",
      "PF01325 :  0.000\n",
      "PF03063 :  0.000\n",
      "PF13487 :  -0.000\n",
      "PF07238 :  -0.000\n",
      "PF12670 :  0.018\n",
      "PF13302 :  -0.003\n",
      "PF01841 :  -0.004\n",
      "PF01882 :  -0.000\n",
      "PF07726 :  -0.001\n",
      "PF14360 :  0.000\n",
      "PF00860 :  0.015\n",
      "PF13462 :  -0.000\n",
      "PF12804 :  -0.000\n",
      "PF00905 :  -0.000\n",
      "PF06100 :  -0.004\n",
      "PF10518 :  0.002\n",
      "PF02872 :  -0.005\n",
      "PF12673 :  -0.005\n",
      "PF03591 :  -0.004\n",
      "PF05437 :  -0.003\n",
      "PF00941 :  -0.003\n",
      "PF03450 :  -0.008\n",
      "PF00483 :  -0.000\n",
      "PF12844 :  0.005\n",
      "PF01381 :  0.015\n",
      "PF13560 :  0.009\n",
      "PF00383 :  0.000\n",
      "PF01872 :  -0.000\n",
      "PF00926 :  0.000\n",
      "PF08282 :  -0.003\n",
      "PF02699 :  0.004\n",
      "PF07992 :  0.010\n",
      "PF00070 :  0.010\n",
      "PF14691 :  -0.003\n",
      "PF10418 :  -0.000\n",
      "PF09285 :  0.000\n",
      "PF01132 :  -0.000\n",
      "PF08207 :  0.000\n",
      "PF00557 :  -0.000\n",
      "PF01321 :  -0.000\n",
      "PF01220 :  -0.000\n",
      "PF13242 :  -0.008\n",
      "PF08264 :  -0.000\n",
      "PF00133 :  -0.000\n",
      "PF13603 :  -0.000\n",
      "PF13704 :  -0.003\n",
      "PF09835 :  0.002\n",
      "PF05175 :  0.007\n",
      "PF13847 :  0.003\n",
      "PF13649 :  0.010\n",
      "PF08241 :  0.003\n",
      "PF08447 :  -0.002\n",
      "PF01590 :  -0.011\n",
      "PF02627 :  0.000\n",
      "PF12682 :  -0.001\n",
      "PF01738 :  0.001\n",
      "PF05448 :  -0.000\n",
      "PF07859 :  -0.000\n",
      "PF00135 :  0.000\n",
      "PF13229 :  0.000\n",
      "PF00682 :  0.003\n",
      "PF00089 :  0.000\n",
      "PF14572 :  -0.000\n",
      "PF13793 :  -0.000\n",
      "PF03372 :  0.000\n",
      "PF13443 :  0.004\n",
      "PF03006 :  -0.000\n",
      "PF02622 :  -0.000\n",
      "PF04519 :  -0.001\n",
      "PF00977 :  -0.002\n",
      "PF02594 :  -0.002\n",
      "PF01025 :  -0.000\n",
      "PF00226 :  0.000\n",
      "PF01556 :  -0.002\n",
      "PF00684 :  0.000\n",
      "PF04452 :  -0.002\n",
      "PF02152 :  -0.002\n",
      "PF00494 :  -0.005\n",
      "PF00717 :  0.002\n",
      "PF02913 :  -0.002\n",
      "PF01565 :  -0.001\n",
      "PF11984 :  -0.000\n",
      "PF09721 :  -0.000\n",
      "PF01386 :  0.006\n",
      "PF14693 :  0.005\n",
      "PF00475 :  0.001\n",
      "PF13241 :  -0.000\n",
      "PF01379 :  -0.001\n",
      "PF03900 :  -0.001\n",
      "PF00590 :  -0.011\n",
      "PF02602 :  -0.001\n",
      "PF00490 :  -0.002\n",
      "PF00202 :  -0.005\n",
      "PF02082 :  0.000\n",
      "PF03444 :  0.000\n",
      "PF01053 :  -0.001\n",
      "PF01041 :  -0.001\n",
      "PF03460 :  0.000\n",
      "PF01077 :  0.000\n",
      "PF12837 :  0.006\n",
      "PF00037 :  0.029\n",
      "PF12838 :  0.004\n",
      "PF13187 :  0.001\n",
      "PF13237 :  0.002\n",
      "PF01206 :  0.002\n",
      "PF00085 :  0.000\n",
      "PF13098 :  0.000\n",
      "PF07449 :  0.000\n",
      "PF02597 :  0.000\n",
      "PF01398 :  0.000\n",
      "PF14464 :  0.000\n",
      "PF13738 :  0.005\n",
      "PF02910 :  -0.000\n",
      "PF12797 :  -0.005\n",
      "PF14697 :  -0.001\n",
      "PF13370 :  0.000\n",
      "PF12800 :  0.004\n",
      "PF01507 :  -0.000\n",
      "PF00009 :  -0.001\n",
      "PF05154 :  -0.000\n",
      "PF02870 :  0.000\n",
      "PF01035 :  0.000\n",
      "PF13581 :  -0.000\n",
      "PF01740 :  0.000\n",
      "PF13466 :  -0.000\n",
      "PF00149 :  -0.001\n",
      "PF12850 :  -0.000\n",
      "PF12320 :  -0.000\n",
      "PF13514 :  0.000\n",
      "PF13555 :  -0.000\n",
      "PF13558 :  -0.000\n",
      "PF02535 :  0.000\n",
      "PF00122 :  0.002\n",
      "PF13630 :  -0.000\n",
      "PF07853 :  -0.000\n",
      "PF12840 :  0.000\n",
      "PF01022 :  0.000\n",
      "PF03952 :  -0.001\n",
      "PF00113 :  -0.001\n",
      "PF13378 :  -0.001\n",
      "PF07476 :  -0.001\n",
      "PF13508 :  0.002\n",
      "PF01757 :  0.000\n",
      "PF04464 :  0.000\n",
      "PF04260 :  -0.000\n",
      "PF13420 :  -0.001\n",
      "PF13673 :  -0.006\n",
      "PF08445 :  -0.003\n",
      "PF14492 :  -0.000\n",
      "PF03764 :  -0.000\n",
      "PF00679 :  -0.001\n",
      "PF05991 :  -0.000\n",
      "PF00480 :  -0.000\n",
      "PF00589 :  0.002\n",
      "PF04854 :  0.000\n",
      "PF13495 :  -0.001\n",
      "PF01944 :  -0.000\n",
      "PF03807 :  0.001\n",
      "PF03446 :  0.000\n",
      "PF01702 :  0.002\n",
      "PF02687 :  0.002\n",
      "PF00465 :  -0.001\n",
      "PF13685 :  -0.000\n",
      "PF01663 :  0.000\n",
      "PF13840 :  0.000\n",
      "PF05173 :  0.000\n",
      "PF01113 :  0.000\n",
      "PF00701 :  0.000\n",
      "PF02774 :  0.000\n",
      "PF01118 :  0.000\n",
      "PF02547 :  0.000\n",
      "PF01012 :  -0.000\n",
      "PF00766 :  -0.000\n",
      "PF00881 :  0.000\n",
      "PF02771 :  0.001\n",
      "PF02770 :  -0.000\n",
      "PF00441 :  0.000\n",
      "PF08028 :  0.000\n",
      "PF16197 :  0.000\n",
      "PF01136 :  -0.000\n",
      "PF02543 :  0.000\n",
      "PF16861 :  0.000\n",
      "PF02737 :  0.000\n",
      "PF00725 :  0.001\n",
      "PF00698 :  0.002\n",
      "PF00300 :  -0.000\n",
      "PF02310 :  -0.011\n",
      "PF01261 :  0.000\n",
      "PF00356 :  -0.001\n",
      "PF00248 :  -0.000\n",
      "PF08240 :  0.000\n",
      "PF00107 :  0.000\n",
      "PF02837 :  -0.000\n",
      "PF02836 :  -0.000\n",
      "PF14681 :  -0.000\n",
      "PF16874 :  0.000\n",
      "PF02065 :  0.000\n",
      "PF16875 :  0.000\n",
      "PF04041 :  0.000\n",
      "PF16011 :  0.000\n",
      "PF05891 :  0.000\n",
      "PF05889 :  -0.000\n",
      "PF00266 :  0.000\n",
      "PF02347 :  0.000\n",
      "PF02646 :  0.000\n",
      "PF00528 :  0.005\n",
      "PF00497 :  -0.001\n",
      "PF00728 :  -0.010\n",
      "PF02838 :  -0.009\n",
      "PF00571 :  -0.005\n",
      "PF13185 :  -0.002\n",
      "PF07335 :  -0.002\n",
      "PF01061 :  -0.001\n",
      "PF13365 :  0.000\n",
      "PF12773 :  -0.000\n",
      "PF04545 :  -0.002\n",
      "PF13463 :  0.001\n",
      "PF00392 :  -0.004\n",
      "PF11799 :  0.000\n",
      "PF00817 :  0.000\n",
      "PF04326 :  0.000\n",
      "PF13749 :  0.000\n",
      "PF07486 :  -0.000\n",
      "PF06969 :  -0.000\n",
      "PF03144 :  -0.001\n",
      "PF16658 :  -0.000\n",
      "PF01212 :  0.001\n",
      "PF07670 :  -0.000\n",
      "PF07664 :  -0.000\n",
      "PF04023 :  -0.001\n",
      "PF14478 :  0.000\n",
      "PF02361 :  0.000\n",
      "PF03773 :  0.000\n",
      "PF13192 :  -0.000\n",
      "PF08242 :  -0.047\n",
      "PF00398 :  0.000\n",
      "PF01795 :  0.000\n",
      "PF00672 :  0.000\n",
      "PF02321 :  0.002\n",
      "PF11604 :  0.000\n",
      "PF00529 :  0.002\n",
      "PF16576 :  0.002\n",
      "PF16572 :  0.000\n",
      "PF13437 :  -0.008\n",
      "PF00873 :  0.000\n",
      "PF00324 :  0.000\n",
      "PF13520 :  0.000\n",
      "PF04237 :  0.000\n",
      "PF06643 :  0.000\n",
      "PF04107 :  0.000\n",
      "PF01848 :  0.000\n",
      "PF02706 :  0.000\n",
      "PF01497 :  0.000\n",
      "PF00425 :  -0.000\n",
      "PF00857 :  0.000\n",
      "PF02554 :  0.000\n",
      "PF13722 :  0.000\n",
      "PF04328 :  0.000\n",
      "PF02195 :  -0.005\n",
      "PF11922 :  0.000\n",
      "PF00582 :  -0.000\n",
      "PF01272 :  -0.001\n",
      "PF14760 :  -0.000\n",
      "PF00445 :  0.000\n",
      "PF00939 :  -0.000\n",
      "PF03600 :  -0.000\n",
      "PF01874 :  -0.000\n",
      "PF03802 :  -0.000\n",
      "PF16193 :  0.000\n",
      "PF12002 :  0.000\n",
      "PF02403 :  0.003\n",
      "PF00587 :  -0.005\n",
      "PF04879 :  -0.002\n",
      "PF00384 :  0.000\n",
      "PF01568 :  0.000\n",
      "PF13247 :  0.000\n",
      "PF12798 :  0.000\n",
      "PF04976 :  0.000\n",
      "PF06779 :  0.000\n",
      "PF00083 :  -0.001\n",
      "PF01228 :  -0.000\n",
      "PF02901 :  -0.000\n",
      "PF01226 :  -0.000\n",
      "PF02624 :  -0.000\n",
      "PF04239 :  -0.001\n",
      "PF00275 :  -0.001\n",
      "PF01435 :  0.000\n",
      "PF13189 :  0.000\n",
      "PF02224 :  -0.000\n",
      "PF03772 :  0.001\n",
      "PF13439 :  0.000\n",
      "PF02655 :  -0.000\n",
      "PF05116 :  -0.001\n",
      "PF13936 :  -0.000\n",
      "PF00665 :  -0.001\n",
      "PF04647 :  -0.000\n",
      "PF00588 :  0.000\n",
      "PF01547 :  0.000\n",
      "PF13416 :  -0.000\n",
      "PF04539 :  -0.003\n",
      "PF12710 :  -0.000\n",
      "PF01195 :  -0.027\n",
      "PF02559 :  0.004\n",
      "PF04851 :  -0.039\n",
      "PF00270 :  -0.047\n",
      "PF00271 :  0.004\n",
      "PF03461 :  0.005\n",
      "PF13145 :  0.000\n",
      "PF13616 :  -0.001\n",
      "PF00639 :  -0.001\n",
      "PF03215 :  0.000\n",
      "PF01520 :  0.000\n",
      "PF00154 :  -0.000\n",
      "PF06745 :  -0.000\n",
      "PF13481 :  0.000\n",
      "PF12704 :  0.001\n",
      "PF01502 :  -0.000\n",
      "PF07685 :  -0.000\n",
      "PF01425 :  -0.000\n",
      "PF00144 :  0.000\n",
      "PF12697 :  -0.007\n",
      "PF08020 :  0.000\n",
      "PF13356 :  0.000\n",
      "PF01709 :  0.000\n",
      "PF01048 :  -0.000\n",
      "PF10423 :  -0.000\n",
      "PF13347 :  -0.000\n",
      "PF01075 :  0.000\n",
      "PF02369 :  0.000\n",
      "PF09134 :  0.000\n",
      "PF11924 :  -0.000\n",
      "PF10017 :  -0.004\n",
      "PF01209 :  -0.005\n",
      "PF00108 :  0.009\n",
      "PF14659 :  0.000\n",
      "PF06167 :  0.000\n",
      "PF05930 :  -0.000\n",
      "PF07733 :  0.003\n",
      "PF02811 :  0.003\n",
      "PF05147 :  -0.003\n",
      "PF13575 :  0.000\n",
      "PF01643 :  -0.000\n",
      "PF07729 :  0.000\n",
      "PF01476 :  -0.001\n",
      "PF05401 :  -0.005\n",
      "PF02742 :  -0.001\n",
      "PF02661 :  -0.000\n",
      "PF00132 :  -0.013\n",
      "PF09989 :  -0.000\n",
      "PF01869 :  -0.000\n",
      "PF03435 :  0.000\n",
      "PF13534 :  -0.000\n",
      "PF13602 :  0.000\n",
      "PF01551 :  -0.008\n",
      "PF02163 :  -0.000\n",
      "PF02075 :  -0.001\n",
      "PF01330 :  -0.001\n",
      "PF14520 :  -0.001\n",
      "PF07499 :  -0.000\n",
      "PF01134 :  0.002\n",
      "PF12831 :  -0.001\n",
      "PF03401 :  0.000\n",
      "PF01970 :  0.000\n",
      "PF12911 :  0.000\n",
      "PF00496 :  -0.002\n",
      "PF08352 :  0.000\n",
      "PF03483 :  0.000\n",
      "PF04069 :  0.000\n",
      "PF02613 :  0.000\n",
      "PF01380 :  -0.003\n",
      "PF01418 :  -0.003\n",
      "PF04074 :  -0.000\n",
      "PF00474 :  -0.004\n",
      "PF04131 :  0.000\n",
      "PF02581 :  0.000\n",
      "PF02743 :  0.000\n",
      "PF01546 :  0.000\n",
      "PF05343 :  0.000\n",
      "PF00912 :  0.000\n",
      "PF01202 :  0.000\n",
      "PF01613 :  -0.000\n",
      "PF02833 :  0.000\n",
      "PF07085 :  0.000\n",
      "PF02446 :  0.000\n",
      "PF00343 :  0.000\n",
      "PF00082 :  0.000\n",
      "PF12784 :  -0.001\n",
      "PF01909 :  -0.000\n",
      "PF08780 :  0.000\n",
      "PF00690 :  0.000\n",
      "PF13246 :  0.000\n",
      "PF00689 :  0.000\n",
      "PF04474 :  0.000\n",
      "PF04454 :  0.002\n",
      "PF09950 :  -0.003\n",
      "PF13426 :  0.001\n",
      "PF01964 :  0.001\n",
      "PF02080 :  0.001\n",
      "PF01434 :  0.000\n",
      "PF06480 :  0.000\n",
      "PF00849 :  -0.001\n",
      "PF04060 :  0.000\n",
      "PF02508 :  0.000\n",
      "PF04205 :  0.000\n",
      "PF03116 :  0.000\n",
      "PF11681 :  0.002\n",
      "PF11863 :  0.007\n",
      "PF13262 :  -0.005\n",
      "PF09979 :  0.013\n",
      "PF04233 :  -0.004\n",
      "PF14206 :  -0.001\n",
      "PF00455 :  -0.001\n",
      "PF08220 :  -0.001\n",
      "PF01174 :  -0.000\n",
      "PF16927 :  0.000\n",
      "PF00403 :  0.004\n",
      "PF12679 :  0.001\n",
      "PF12730 :  0.000\n",
      "PF03960 :  -0.000\n",
      "PF07179 :  0.000\n",
      "PF04101 :  -0.000\n",
      "PF13385 :  -0.000\n",
      "PF08522 :  -0.003\n",
      "PF16141 :  -0.000\n",
      "PF12741 :  -0.000\n",
      "PF12771 :  -0.010\n",
      "PF07660 :  -0.000\n",
      "PF16344 :  0.000\n",
      "PF04773 :  -0.000\n",
      "PF00196 :  0.000\n",
      "PF07638 :  0.000\n",
      "PF08973 :  0.000\n",
      "PF13248 :  -0.000\n",
      "PF04892 :  0.000\n",
      "PF13459 :  -0.000\n",
      "PF07610 :  0.000\n",
      "PF12849 :  0.000\n",
      "PF00989 :  0.000\n",
      "PF01595 :  -0.000\n",
      "PF03471 :  0.003\n",
      "PF00437 :  -0.000\n",
      "PF02800 :  0.000\n",
      "PF00044 :  0.000\n",
      "PF06177 :  0.000\n",
      "PF01558 :  0.016\n",
      "PF01855 :  0.019\n",
      "PF02776 :  0.011\n",
      "PF01063 :  0.005\n",
      "PF04026 :  0.023\n",
      "PF01225 :  0.000\n",
      "PF08245 :  0.000\n",
      "PF02875 :  0.000\n",
      "PF07261 :  0.000\n",
      "PF15418 :  0.000\n",
      "PF13614 :  0.000\n",
      "PF10609 :  0.000\n",
      "PF06564 :  0.000\n",
      "PF09140 :  0.000\n",
      "PF01656 :  -0.001\n",
      "PF00142 :  0.000\n",
      "PF02374 :  0.000\n",
      "PF01464 :  0.001\n",
      "PF01411 :  0.000\n",
      "PF07973 :  -0.000\n",
      "PF13538 :  -0.002\n",
      "PF00174 :  0.000\n",
      "PF01794 :  0.000\n",
      "PF09223 :  0.000\n",
      "PF02283 :  0.000\n",
      "PF12848 :  0.005\n",
      "PF02810 :  -0.000\n",
      "PF12974 :  0.000\n",
      "PF02110 :  -0.000\n",
      "PF08211 :  0.000\n",
      "PF01832 :  -0.000\n",
      "PF10988 :  -0.007\n",
      "PF10412 :  0.000\n",
      "PF12696 :  0.000\n",
      "PF03050 :  -0.001\n",
      "PF03412 :  -0.003\n",
      "PF01624 :  -0.001\n",
      "PF00145 :  -0.000\n",
      "PF03703 :  -0.000\n",
      "PF13155 :  -0.012\n",
      "PF07751 :  -0.000\n",
      "PF05717 :  -0.000\n",
      "PF08443 :  -0.000\n",
      "PF02955 :  -0.000\n",
      "PF10458 :  -0.000\n",
      "PF08643 :  -0.000\n",
      "PF04986 :  0.000\n",
      "PF00015 :  0.002\n",
      "PF09587 :  -0.000\n",
      "PF03816 :  0.000\n",
      "PF00933 :  -0.001\n",
      "PF01915 :  -0.001\n",
      "PF13375 :  0.000\n",
      "PF05896 :  0.000\n",
      "PF01512 :  -0.001\n",
      "PF10531 :  0.000\n",
      "PF13204 :  0.002\n",
      "PF12904 :  0.003\n",
      "PF01263 :  -0.004\n",
      "PF00408 :  0.000\n",
      "PF02880 :  0.000\n",
      "PF02879 :  -0.010\n",
      "PF02878 :  0.000\n",
      "PF16109 :  0.000\n",
      "PF13567 :  0.000\n",
      "PF00834 :  -0.000\n",
      "PF02911 :  -0.001\n",
      "PF00551 :  -0.028\n",
      "PF01300 :  -0.009\n",
      "PF09992 :  -0.016\n",
      "PF12668 :  0.000\n",
      "PF13151 :  0.000\n",
      "PF01925 :  0.000\n",
      "PF03486 :  0.002\n",
      "PF03022 :  0.000\n",
      "PF03606 :  0.000\n",
      "PF06808 :  0.000\n",
      "PF01810 :  0.000\n",
      "PF01946 :  0.001\n",
      "PF01490 :  0.000\n",
      "PF02458 :  0.000\n",
      "PF01668 :  -0.000\n",
      "PF02588 :  -0.001\n",
      "PF10035 :  -0.000\n",
      "PF05860 :  0.000\n",
      "PF05594 :  -0.000\n",
      "PF08479 :  -0.000\n",
      "PF17287 :  0.000\n",
      "PF03865 :  -0.000\n",
      "PF02826 :  -0.000\n",
      "PF01661 :  0.000\n",
      "PF13091 :  -0.001\n",
      "PF08032 :  0.000\n",
      "PF13432 :  0.000\n",
      "PF04733 :  0.000\n",
      "PF14559 :  0.000\n",
      "PF01042 :  0.000\n",
      "PF13638 :  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF02562 :  0.000\n",
      "PF13604 :  -0.000\n",
      "PF09848 :  0.000\n",
      "PF13727 :  0.000\n",
      "PF07228 :  0.000\n",
      "PF00400 :  0.000\n",
      "PF01471 :  -0.001\n",
      "PF13744 :  0.006\n",
      "PF05973 :  0.000\n",
      "PF02436 :  -0.000\n",
      "PF00364 :  -0.000\n",
      "PF10704 :  0.000\n",
      "PF13477 :  0.000\n",
      "PF13579 :  -0.001\n",
      "PF02583 :  0.000\n",
      "PF00830 :  0.000\n",
      "PF17293 :  0.000\n",
      "PF13102 :  0.000\n",
      "PF01637 :  -0.000\n",
      "PF13173 :  0.000\n",
      "PF01609 :  0.001\n",
      "PF05598 :  -0.000\n",
      "PF11756 :  -0.000\n",
      "PF13506 :  0.000\n",
      "PF04240 :  0.000\n",
      "PF06827 :  0.000\n",
      "PF06831 :  0.000\n",
      "PF02769 :  0.000\n",
      "PF00920 :  0.000\n",
      "PF01026 :  -0.003\n",
      "PF04909 :  0.005\n",
      "PF13728 :  -0.001\n",
      "PF00657 :  0.000\n",
      "PF13930 :  0.000\n",
      "PF03796 :  0.000\n",
      "PF12762 :  0.000\n",
      "PF00401 :  0.000\n",
      "PF02823 :  0.000\n",
      "PF02894 :  0.000\n",
      "PF02746 :  0.000\n",
      "PF04204 :  0.000\n",
      "PF04072 :  -0.003\n",
      "PF14765 :  0.000\n",
      "PF00140 :  0.000\n",
      "PF03979 :  0.000\n",
      "PF13362 :  -0.000\n",
      "PF13662 :  -0.007\n",
      "PF08275 :  -0.000\n",
      "PF01807 :  -0.000\n",
      "PF00488 :  -0.000\n",
      "PF00436 :  0.001\n",
      "PF00694 :  -0.000\n",
      "PF00330 :  -0.000\n",
      "PF00892 :  0.000\n",
      "PF08238 :  0.000\n",
      "PF00999 :  -0.007\n",
      "PF04226 :  0.000\n",
      "PF00205 :  0.000\n",
      "PF10369 :  0.000\n",
      "PF08502 :  0.000\n",
      "PF00180 :  0.000\n",
      "PF00150 :  0.002\n",
      "PF13522 :  0.000\n",
      "PF06472 :  0.000\n",
      "PF03432 :  -0.001\n",
      "PF00239 :  0.000\n",
      "PF07508 :  0.000\n",
      "PF13408 :  0.000\n",
      "PF14287 :  0.000\n",
      "PF02638 :  -0.004\n",
      "PF00563 :  -0.002\n",
      "PF09823 :  0.002\n",
      "PF00078 :  -0.000\n",
      "PF08291 :  0.000\n",
      "PF12224 :  -0.000\n",
      "PF02580 :  -0.002\n",
      "PF07862 :  0.000\n",
      "PF07963 :  0.000\n",
      "PF03977 :  -0.000\n",
      "PF00814 :  -0.000\n",
      "PF02367 :  -0.000\n",
      "PF02348 :  0.000\n",
      "PF03323 :  0.001\n",
      "PF03845 :  0.001\n",
      "PF00464 :  0.001\n",
      "PF13533 :  -0.006\n",
      "PF13336 :  0.000\n",
      "PF02550 :  -0.000\n",
      "PF07478 :  0.000\n",
      "PF02786 :  0.000\n",
      "PF03129 :  0.000\n",
      "PF00707 :  0.000\n",
      "PF00453 :  0.000\n",
      "PF05491 :  0.000\n",
      "PF11188 :  0.000\n",
      "PF17147 :  0.006\n",
      "PF01008 :  0.000\n",
      "PF12698 :  0.001\n",
      "PF00809 :  0.000\n",
      "PF09586 :  0.000\n",
      "PF12715 :  0.000\n",
      "PF13393 :  0.000\n",
      "PF02492 :  0.000\n",
      "PF00708 :  0.000\n",
      "PF07503 :  0.000\n",
      "PF01455 :  0.000\n",
      "PF01924 :  0.000\n",
      "PF00586 :  0.000\n",
      "PF00012 :  0.000\n",
      "PF06723 :  0.000\n",
      "PF00166 :  0.000\n",
      "PF00768 :  -0.000\n",
      "PF13354 :  -0.000\n",
      "PF14057 :  0.000\n",
      "PF00691 :  -0.000\n",
      "PF13458 :  -0.001\n",
      "PF12385 :  0.005\n",
      "PF00916 :  0.001\n",
      "PF02915 :  -0.006\n",
      "PF00877 :  0.000\n",
      "PF12650 :  -0.000\n",
      "PF02353 :  0.000\n",
      "PF07589 :  -0.000\n",
      "PF03449 :  -0.000\n",
      "PF01259 :  -0.000\n",
      "PF00636 :  -0.000\n",
      "PF08239 :  -0.000\n",
      "PF06347 :  0.000\n",
      "PF13180 :  0.000\n",
      "PF00595 :  0.000\n",
      "PF11007 :  -0.000\n",
      "PF12652 :  -0.000\n",
      "PF05067 :  -0.000\n",
      "PF07944 :  -0.002\n",
      "PF10000 :  0.000\n",
      "PF12637 :  -0.000\n",
      "PF03070 :  0.000\n",
      "PF01256 :  -0.001\n",
      "PF00389 :  -0.000\n",
      "PF01958 :  -0.000\n",
      "PF03447 :  0.000\n",
      "PF00478 :  -0.000\n",
      "PF01070 :  0.000\n",
      "PF02384 :  0.000\n",
      "PF05954 :  0.000\n",
      "PF17541 :  0.000\n",
      "PF02368 :  0.000\n",
      "PF00793 :  -0.000\n",
      "PF01761 :  -0.000\n",
      "PF14602 :  -0.002\n",
      "PF13648 :  0.000\n",
      "PF05593 :  -0.001\n",
      "PF01479 :  0.000\n",
      "PF00724 :  -0.001\n",
      "PF02861 :  0.000\n",
      "PF12464 :  -0.000\n",
      "PF13580 :  -0.001\n",
      "PF14446 :  0.000\n",
      "PF04203 :  0.000\n",
      "PF01475 :  0.000\n",
      "PF02922 :  -0.000\n",
      "PF13245 :  -0.000\n",
      "PF08840 :  0.000\n",
      "PF13382 :  0.000\n",
      "PF01979 :  0.000\n",
      "PF01297 :  0.000\n",
      "PF00950 :  0.000\n",
      "PF02498 :  -0.001\n",
      "PF01145 :  -0.001\n",
      "PF01594 :  0.000\n",
      "PF01262 :  0.000\n",
      "PF01488 :  0.000\n",
      "PF02632 :  0.000\n",
      "PF03553 :  0.000\n",
      "PF13434 :  -0.001\n",
      "PF13589 :  -0.003\n",
      "PF14028 :  -0.002\n",
      "PF04738 :  -0.002\n",
      "PF13333 :  -0.000\n",
      "PF13683 :  -0.000\n",
      "PF00069 :  0.000\n",
      "PF07714 :  0.000\n",
      "PF02222 :  0.000\n",
      "PF01894 :  0.000\n",
      "PF00176 :  0.000\n",
      "PF13823 :  0.000\n",
      "PF04304 :  -0.000\n",
      "PF00765 :  -0.000\n",
      "PF06276 :  -0.001\n",
      "PF04183 :  -0.001\n",
      "PF12832 :  0.000\n",
      "PF00696 :  0.000\n",
      "PF02378 :  -0.002\n",
      "PF07681 :  -0.000\n",
      "PF00884 :  0.000\n",
      "PF07244 :  0.000\n",
      "PF01103 :  0.000\n",
      "PF03033 :  -0.000\n",
      "PF01588 :  0.000\n",
      "PF07683 :  0.000\n",
      "PF00175 :  0.000\n",
      "PF13018 :  -0.000\n",
      "PF07581 :  0.003\n",
      "PF00171 :  -0.001\n",
      "PF13505 :  -0.000\n",
      "PF13905 :  0.000\n",
      "PF10294 :  0.000\n",
      "PF06325 :  -0.000\n",
      "PF01081 :  -0.000\n",
      "PF02592 :  -0.000\n",
      "PF00313 :  0.000\n",
      "PF00255 :  0.000\n",
      "PF00209 :  0.000\n",
      "PF13243 :  -0.003\n",
      "PF00432 :  -0.004\n",
      "PF13249 :  0.007\n",
      "PF00289 :  0.000\n",
      "PF02785 :  0.000\n",
      "PF01039 :  -0.000\n",
      "PF05257 :  0.000\n",
      "PF07702 :  0.000\n",
      "PF01171 :  0.000\n",
      "PF02302 :  0.000\n",
      "PF01467 :  0.000\n",
      "PF07338 :  -0.000\n",
      "PF16715 :  -0.002\n",
      "PF05746 :  0.000\n",
      "PF12895 :  0.000\n",
      "PF03102 :  0.000\n",
      "PF05159 :  0.000\n",
      "PF03780 :  0.000\n",
      "PF02867 :  -0.000\n",
      "PF00317 :  -0.000\n",
      "PF00893 :  0.000\n",
      "PF02517 :  0.000\n",
      "PF05192 :  -0.000\n",
      "PF05190 :  -0.000\n",
      "PF05188 :  -0.000\n",
      "PF06133 :  -0.000\n",
      "PF01938 :  -0.000\n",
      "PF00919 :  -0.000\n",
      "PF08423 :  -0.000\n",
      "PF06381 :  -0.006\n",
      "PF02534 :  0.000\n",
      "PF13306 :  -0.000\n",
      "PF07516 :  0.000\n",
      "PF07517 :  -0.000\n",
      "PF01043 :  -0.000\n",
      "PF02086 :  0.000\n",
      "PF06293 :  0.000\n",
      "PF02092 :  0.000\n",
      "PF10439 :  0.000\n",
      "PF00953 :  0.000\n",
      "PF02686 :  -0.002\n",
      "PF02934 :  -0.002\n",
      "PF02637 :  -0.002\n",
      "PF01728 :  0.000\n",
      "PF01699 :  -0.000\n",
      "PF01098 :  -0.000\n",
      "PF00498 :  0.000\n",
      "PF06245 :  -0.000\n",
      "PF07221 :  -0.000\n",
      "PF03853 :  -0.000\n",
      "PF06050 :  -0.000\n",
      "PF13635 :  0.000\n",
      "PF01252 :  0.000\n",
      "PF03720 :  0.000\n",
      "PF10694 :  -0.000\n",
      "PF07947 :  -0.000\n",
      "PF04290 :  0.000\n",
      "PF03480 :  0.000\n",
      "PF08786 :  -0.000\n",
      "PF11659 :  0.000\n",
      "PF03741 :  -0.000\n",
      "PF01653 :  -0.000\n",
      "PF03120 :  -0.000\n",
      "PF12826 :  -0.000\n",
      "PF01976 :  0.012\n",
      "PF00348 :  0.014\n",
      "PF13360 :  -0.009\n",
      "PF12801 :  -0.004\n",
      "PF13570 :  -0.006\n",
      "PF01257 :  -0.004\n",
      "PF10589 :  -0.005\n",
      "PF13510 :  -0.006\n",
      "PF06902 :  -0.004\n",
      "PF08486 :  -0.001\n",
      "PF01634 :  -0.000\n",
      "PF01610 :  0.000\n",
      "PF03852 :  0.000\n",
      "PF01527 :  0.000\n",
      "PF03458 :  0.000\n",
      "PF04392 :  0.000\n",
      "PF00772 :  0.000\n",
      "PF13672 :  -0.000\n",
      "PF09719 :  0.000\n",
      "PF00011 :  -0.000\n",
      "PF14579 :  0.002\n",
      "PF13004 :  0.000\n",
      "PF13899 :  0.000\n",
      "PF05768 :  -0.000\n",
      "PF01695 :  0.000\n",
      "PF14622 :  0.000\n",
      "PF14198 :  0.000\n",
      "PF02608 :  0.000\n",
      "PF13343 :  0.000\n",
      "PF00079 :  -0.000\n",
      "PF00121 :  0.000\n",
      "PF00162 :  0.000\n",
      "PF02386 :  -0.000\n",
      "PF07943 :  -0.000\n",
      "PF01676 :  0.000\n",
      "PF05970 :  -0.000\n",
      "PF05127 :  -0.000\n",
      "PF14490 :  -0.000\n",
      "PF01957 :  -0.001\n",
      "PF02357 :  -0.000\n",
      "PF01116 :  0.000\n",
      "PF03611 :  0.000\n",
      "PF00359 :  -0.000\n",
      "PF08753 :  0.000\n",
      "PF01402 :  -0.000\n",
      "PF03668 :  -0.000\n",
      "PF01933 :  -0.000\n",
      "PF10298 :  -0.000\n",
      "PF14527 :  -0.000\n",
      "PF02650 :  -0.000\n",
      "PF00493 :  0.000\n",
      "PF09445 :  0.000\n",
      "PF03602 :  0.000\n",
      "PF07009 :  0.000\n",
      "PF00882 :  -0.002\n",
      "PF14285 :  -0.000\n",
      "PF02424 :  0.000\n",
      "PF13518 :  0.000\n",
      "PF13384 :  0.000\n",
      "PF03717 :  0.000\n",
      "PF03255 :  0.000\n",
      "PF12327 :  -0.001\n",
      "PF00091 :  -0.002\n",
      "PF13386 :  0.000\n",
      "PF01148 :  0.000\n",
      "PF17415 :  0.000\n",
      "PF12667 :  0.000\n",
      "PF02684 :  0.000\n",
      "PF01975 :  0.000\n",
      "PF02590 :  -0.000\n",
      "PF08645 :  -0.000\n",
      "PF06833 :  0.000\n",
      "PF02803 :  -0.000\n",
      "PF01996 :  0.000\n",
      "PF03193 :  0.000\n",
      "PF16745 :  0.000\n",
      "PF03793 :  0.000\n",
      "PF14531 :  0.000\n",
      "PF00481 :  -0.001\n",
      "PF01189 :  0.001\n",
      "PF01029 :  0.000\n",
      "PF02223 :  0.000\n",
      "PF01177 :  0.000\n",
      "PF01135 :  0.000\n",
      "PF06962 :  -0.000\n",
      "PF03419 :  -0.000\n",
      "PF09639 :  -0.001\n",
      "PF03374 :  -0.001\n",
      "PF04166 :  0.000\n",
      "PF02537 :  0.000\n",
      "PF01741 :  0.000\n",
      "PF02645 :  0.000\n",
      "PF00614 :  -0.001\n",
      "PF13396 :  -0.001\n",
      "PF04999 :  0.000\n",
      "PF02381 :  0.000\n",
      "PF12793 :  0.000\n",
      "PF07969 :  0.000\n",
      "PF01255 :  0.000\n",
      "PF01765 :  0.000\n",
      "PF00710 :  -0.000\n",
      "PF15617 :  -0.000\n",
      "PF06738 :  0.000\n",
      "PF12821 :  0.000\n",
      "PF06421 :  0.000\n",
      "PF11589 :  0.000\n",
      "PF07884 :  0.000\n",
      "PF13817 :  -0.000\n",
      "PF01018 :  -0.000\n",
      "PF09269 :  -0.000\n",
      "PF14276 :  -0.000\n",
      "PF04070 :  -0.000\n",
      "PF10410 :  -0.000\n",
      "PF16656 :  -0.000\n",
      "PF13636 :  0.001\n",
      "PF17125 :  0.001\n",
      "PF00929 :  0.000\n",
      "PF02132 :  -0.000\n",
      "PF02575 :  -0.000\n",
      "PF12169 :  0.000\n",
      "PF13177 :  0.000\n",
      "PF13500 :  -0.000\n",
      "PF00572 :  0.000\n",
      "PF00380 :  0.000\n",
      "PF10502 :  0.000\n",
      "PF02475 :  0.000\n",
      "PF02629 :  -0.000\n",
      "PF01501 :  0.000\n",
      "PF04079 :  -0.000\n",
      "PF02616 :  -0.000\n",
      "PF02463 :  0.000\n",
      "PF02683 :  -0.001\n",
      "PF17389 :  0.000\n",
      "PF17390 :  0.000\n",
      "PF07927 :  0.000\n",
      "PF05534 :  0.000\n",
      "PF01095 :  -0.021\n",
      "PF13750 :  -0.002\n",
      "PF05816 :  0.000\n",
      "PF03572 :  0.000\n",
      "PF01040 :  -0.000\n",
      "PF06414 :  0.004\n",
      "PF11167 :  -0.000\n",
      "PF09579 :  -0.000\n",
      "PF08843 :  -0.000\n",
      "PF02371 :  0.001\n",
      "PF01548 :  0.001\n",
      "PF01443 :  -0.000\n",
      "PF09547 :  0.000\n",
      "PF16472 :  -0.006\n",
      "PF00795 :  0.000\n",
      "PF02631 :  -0.000\n",
      "PF08442 :  -0.000\n",
      "PF13549 :  -0.000\n",
      "PF00549 :  -0.000\n",
      "PF13607 :  -0.000\n",
      "PF08348 :  -0.000\n",
      "PF13309 :  -0.000\n",
      "PF00342 :  0.004\n",
      "PF04413 :  0.000\n",
      "PF00815 :  -0.000\n",
      "PF14537 :  -0.000\n",
      "PF02390 :  -0.000\n",
      "PF13484 :  0.004\n",
      "PF04266 :  0.000\n",
      "PF15919 :  0.000\n",
      "PF09546 :  0.000\n",
      "PF06686 :  0.000\n",
      "PF09548 :  0.000\n",
      "PF13676 :  0.001\n",
      "PF07876 :  0.000\n",
      "PF01555 :  -0.003\n",
      "PF13595 :  0.000\n",
      "PF12508 :  0.000\n",
      "PF02863 :  0.000\n",
      "PF01316 :  0.000\n",
      "PF01513 :  0.000\n",
      "PF02609 :  0.000\n",
      "PF02601 :  0.000\n",
      "PF13742 :  0.000\n",
      "PF03799 :  -0.001\n",
      "PF03744 :  -0.000\n",
      "PF13149 :  0.000\n",
      "PF13568 :  0.004\n",
      "PF04610 :  0.000\n",
      "PF06397 :  0.000\n",
      "PF01880 :  0.000\n",
      "PF04893 :  0.002\n",
      "PF03118 :  0.000\n",
      "PF01196 :  0.000\n",
      "PF02225 :  0.000\n",
      "PF03313 :  -0.002\n",
      "PF06574 :  0.000\n",
      "PF01687 :  0.000\n",
      "PF06415 :  0.000\n",
      "PF01472 :  0.000\n",
      "PF04466 :  0.000\n",
      "PF07456 :  0.000\n",
      "PF13428 :  0.000\n",
      "PF00749 :  0.000\n",
      "PF00533 :  -0.000\n",
      "PF16473 :  0.000\n",
      "PF13038 :  0.000\n",
      "PF06964 :  0.000\n",
      "PF08823 :  0.000\n",
      "PF13335 :  -0.000\n",
      "PF07971 :  0.000\n",
      "PF06874 :  0.000\n",
      "PF08353 :  0.000\n",
      "PF12685 :  0.000\n",
      "PF13344 :  0.000\n",
      "PF06434 :  0.000\n",
      "PF07786 :  0.000\n",
      "PF14238 :  0.000\n",
      "PF13451 :  -0.000\n",
      "PF04223 :  -0.000\n",
      "PF14195 :  0.000\n",
      "PF14191 :  0.000\n",
      "PF13610 :  0.000\n",
      "PF08331 :  0.001\n",
      "PF01784 :  0.000\n",
      "PF04693 :  0.001\n",
      "PF13546 :  0.001\n",
      "PF05656 :  0.000\n",
      "PF01739 :  0.000\n",
      "PF05577 :  -0.000\n",
      "PF11028 :  0.000\n",
      "PF10090 :  0.000\n",
      "PF13523 :  -0.001\n",
      "PF02900 :  0.000\n",
      "PF00263 :  0.011\n",
      "PF03315 :  -0.002\n",
      "PF01955 :  -0.001\n",
      "PF13310 :  -0.000\n",
      "PF04231 :  0.000\n",
      "PF04173 :  -0.000\n",
      "PF03180 :  -0.000\n",
      "PF01578 :  0.000\n",
      "PF16327 :  0.000\n",
      "PF13783 :  0.000\n",
      "PF00183 :  0.000\n",
      "PF03484 :  -0.000\n",
      "PF13086 :  -0.000\n",
      "PF09891 :  -0.000\n",
      "PF04993 :  -0.000\n",
      "PF08478 :  -0.000\n",
      "PF02481 :  -0.000\n",
      "PF01396 :  -0.000\n",
      "PF06018 :  -0.000\n",
      "PF08222 :  -0.000\n",
      "PF00318 :  -0.000\n",
      "PF00889 :  -0.000\n",
      "PF13331 :  0.000\n",
      "PF01712 :  0.000\n",
      "PF02591 :  0.000\n",
      "PF11306 :  0.000\n",
      "PF04246 :  0.000\n",
      "PF16286 :  -0.001\n",
      "PF13497 :  0.000\n",
      "PF06406 :  0.000\n",
      "PF10784 :  -0.000\n",
      "PF03515 :  -0.000\n",
      "PF01024 :  0.000\n",
      "PF03857 :  -0.000\n",
      "PF14859 :  -0.000\n",
      "PF13438 :  -0.000\n",
      "PF07395 :  -0.000\n",
      "PF09831 :  -0.000\n",
      "PF10820 :  -0.000\n",
      "PF06519 :  -0.000\n",
      "PF06610 :  0.000\n",
      "PF13710 :  0.000\n",
      "PF03825 :  0.000\n",
      "PF09538 :  0.000\n",
      "PF12128 :  0.000\n",
      "PF03147 :  -0.000\n",
      "PF01575 :  0.000\n",
      "PF04613 :  0.000\n",
      "PF14534 :  -0.000\n",
      "PF07091 :  0.000\n",
      "PF03705 :  0.000\n",
      "PF01339 :  0.000\n",
      "PF04952 :  0.000\n",
      "PF01640 :  0.000\n",
      "PF13734 :  0.000\n",
      "PF05649 :  0.000\n",
      "PF01431 :  0.000\n",
      "PF17288 :  0.000\n",
      "PF02768 :  -0.000\n",
      "PF11672 :  0.000\n",
      "PF02767 :  0.000\n",
      "PF00712 :  0.000\n",
      "PF03092 :  0.000\n",
      "PF16269 :  -0.000\n",
      "PF16125 :  0.000\n",
      "PF04235 :  -0.000\n",
      "PF14114 :  -0.000\n",
      "PF16168 :  0.000\n",
      "PF13148 :  0.000\n",
      "PF11726 :  -0.000\n",
      "PF11682 :  -0.000\n",
      "PF15655 :  0.000\n",
      "PF04531 :  0.000\n",
      "PF05728 :  -0.005\n",
      "PF03590 :  0.000\n",
      "PF05576 :  0.000\n",
      "PF08437 :  0.000\n",
      "PF06176 :  0.000\n",
      "PF08218 :  0.000\n",
      "PF01149 :  0.000\n",
      "PF11198 :  0.000\n",
      "PF05321 :  0.000\n",
      "PF04956 :  0.000\n",
      "PF05101 :  0.000\n",
      "PF03135 :  0.000\n",
      "PF07996 :  0.000\n",
      "PF04335 :  0.000\n",
      "PF03524 :  0.000\n",
      "PF03743 :  0.000\n",
      "PF12864 :  0.000\n",
      "PF01011 :  0.000\n",
      "PF03096 :  0.000\n",
      "PF14810 :  0.000\n",
      "PF14899 :  0.000\n",
      "PF01654 :  0.000\n",
      "PF02322 :  0.000\n",
      "PF11026 :  0.000\n",
      "PF08501 :  0.000\n",
      "PF00709 :  0.000\n",
      "PF05857 :  -0.000\n",
      "PF03958 :  -0.000\n",
      "PF16249 :  0.000\n",
      "PF10543 :  0.000\n",
      "PF04679 :  0.000\n",
      "PF01068 :  0.000\n",
      "PF02586 :  0.000\n",
      "PF14266 :  0.000\n",
      "PF15902 :  0.000\n",
      "PF09822 :  0.000\n",
      "PF03739 :  0.000\n",
      "PF13690 :  0.000\n",
      "PF04509 :  0.000\n",
      "PF10639 :  0.000\n",
      "PF04439 :  0.000\n",
      "PF12412 :  0.000\n",
      "PF16347 :  -0.000\n",
      "PF02995 :  -0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper method to print importances and visualize distribution\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n",
    "visualize_importances(feature_names, np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = []\n",
    "v = []\n",
    "d = {}\n",
    "for i in range(len(feature_names)):\n",
    "    if abs(np.mean(attr, axis=0)[i]) > 0.020:\n",
    "        #print(feature_names[i], \": \", '%.3f'%(np.mean(attr, axis=0)[i]))\n",
    "        f.append(feature_names[i])\n",
    "        v.append(np.mean(attr, axis=0)[i])\n",
    "        d[feature_names[i]] = np.mean(attr, axis=0)[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PF00270': -0.04728373110330848,\n",
       " 'PF08242': -0.046951156905793136,\n",
       " 'PF04397': -0.04226317803803337,\n",
       " 'PF04851': -0.03872440840628483,\n",
       " 'PF00890': -0.03355157093253758,\n",
       " 'PF13561': -0.03041220065844106,\n",
       " 'PF13186': -0.029560812493471336,\n",
       " 'PF13723': -0.02822444091449483,\n",
       " 'PF00551': -0.027719720631829852,\n",
       " 'PF01842': -0.027503797586146298,\n",
       " 'PF00106': -0.027193274063153472,\n",
       " 'PF01195': -0.026660194629982075,\n",
       " 'PF01522': -0.02587006136008979,\n",
       " 'PF13165': -0.02477251583061785,\n",
       " 'PF01095': -0.021359675911911964,\n",
       " 'PF01553': 0.02058163571096759,\n",
       " 'PF13279': 0.02181579518732501,\n",
       " 'PF13620': 0.02304911256881359,\n",
       " 'PF00072': 0.02311099792226795,\n",
       " 'PF04026': 0.02335717178182157,\n",
       " 'PF00593': 0.023436397801030684,\n",
       " 'PF00891': 0.02400163782402036,\n",
       " 'PF02311': 0.02756509246391988,\n",
       " 'PF00037': 0.028726972973244357,\n",
       " 'PF07715': 0.0324702285556711,\n",
       " 'PF00109': 0.033441234710994285,\n",
       " 'PF00535': 0.04649785552947337,\n",
       " 'PF03176': 0.05481890692465908,\n",
       " 'PF02801': 0.06289358166911385}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "sorted_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures_df = pd.DataFrame.from_dict(sorted_d,columns = ['importance'], orient='index').reset_index()\n",
    "feautures_df.rename(columns = {'index': 'pfams'}, inplace = True)\n",
    "feautures_df.to_csv('df_feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.xticks([], d.keys(), rotation='vertical', color = 'gold')\n",
    "#plt.xticks(x, labels, rotation='vertical')\n",
    "#plt.bar(d.keys(), d.values(), width = 0.8, color='r')\n",
    "plt.barh(list(d.keys()), sorted(list(d.values())), align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PF00037',\n",
       " 'PF00072',\n",
       " 'PF00106',\n",
       " 'PF00109',\n",
       " 'PF00270',\n",
       " 'PF00535',\n",
       " 'PF00551',\n",
       " 'PF00593',\n",
       " 'PF00890',\n",
       " 'PF00891',\n",
       " 'PF01095',\n",
       " 'PF01195',\n",
       " 'PF01522',\n",
       " 'PF01553',\n",
       " 'PF01842',\n",
       " 'PF02311',\n",
       " 'PF02801',\n",
       " 'PF03176',\n",
       " 'PF04026',\n",
       " 'PF04397',\n",
       " 'PF04851',\n",
       " 'PF07715',\n",
       " 'PF08242',\n",
       " 'PF13165',\n",
       " 'PF13186',\n",
       " 'PF13279',\n",
       " 'PF13561',\n",
       " 'PF13620',\n",
       " 'PF13723']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
